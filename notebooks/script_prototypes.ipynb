{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ericmjl/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/home/ericmjl/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from pin import pin\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "import json\n",
    "import os\n",
    "import autograd.numpy as np\n",
    "import pickle as pkl\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('../data/batch_summary.json') as f:\n",
    "    model_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'code': 'wJ9TDy',\n",
       " 'created': '2016-01-02',\n",
       " 'models': [{'GMQE': 0.9800000191,\n",
       "   'id': '01',\n",
       "   'oligo_state': 'homo-dimer',\n",
       "   'qmean': 0.3202740713,\n",
       "   'seq_coverage': 1.0,\n",
       "   'seq_id': 91.9191894531,\n",
       "   'seq_sim': 0.5824936032,\n",
       "   'status': 'COMPLETED',\n",
       "   'template': '4ll3.1.B',\n",
       "   'tpl_seq': 'PQITLWQRPLVTIKIGGQLKEALLDTGADDTVLEEMNLPGRWKPKMIGGIGGFIKVRQYDQILIEICGHKAIGTVLVGPTPVNIIGRNLLTQIGCTLNF',\n",
       "   'trg_seq': 'PQITLWQRPLVTIKIEGQLKEALLDTGADDTVLEEINLSGKWKPKMIGGIGGFIKVGQYDQITIEICGHKVIGTVLVGPTPVNIIGRNLLTQLGCTLNF'},\n",
       "  {'GMQE': 0.9800000191,\n",
       "   'id': '02',\n",
       "   'oligo_state': 'homo-dimer',\n",
       "   'qmean': -0.9170268789,\n",
       "   'seq_coverage': 1.0,\n",
       "   'seq_id': 91.9191894531,\n",
       "   'seq_sim': 0.5824936032,\n",
       "   'status': 'COMPLETED',\n",
       "   'template': '1a8g.1.A',\n",
       "   'tpl_seq': 'PQITLWQRPLVTIKIGGQLKEALLDTGADDTVLEEMNLPGRWKPKMIGGIGGFIKVRQYDQILIEICGHKAIGTVLVGPTPVNIIGRNLLTQIGCTLNF',\n",
       "   'trg_seq': 'PQITLWQRPLVTIKIEGQLKEALLDTGADDTVLEEINLSGKWKPKMIGGIGGFIKVGQYDQITIEICGHKVIGTVLVGPTPVNIIGRNLLTQLGCTLNF'},\n",
       "  {'GMQE': 0.9800000191,\n",
       "   'id': '03',\n",
       "   'oligo_state': 'homo-dimer',\n",
       "   'qmean': -0.5994937920000001,\n",
       "   'seq_coverage': 1.0,\n",
       "   'seq_id': 91.9191894531,\n",
       "   'seq_sim': 0.5824936032,\n",
       "   'status': 'COMPLETED',\n",
       "   'template': '1vik.1.A',\n",
       "   'tpl_seq': 'PQITLWQRPLVTIKIGGQLKEALLDTGADDTVLEEMNLPGRWKPKMIGGIGGFIKVRQYDQILIEICGHKAIGTVLVGPTPVNIIGRNLLTQIGCTLNF',\n",
       "   'trg_seq': 'PQITLWQRPLVTIKIEGQLKEALLDTGADDTVLEEINLSGKWKPKMIGGIGGFIKVGQYDQITIEICGHKVIGTVLVGPTPVNIIGRNLLTQLGCTLNF'}],\n",
       " 'target': 'PQITLWQRPLVTIKIEGQLKEALLDTGADDTVLEEINLSGKWKPKMIGGIGGFIKVGQYDQITIEICGHKVIGTVLVGPTPVNIIGRNLLTQLGCTLNF',\n",
       " 'title': '259265-1',\n",
       " 'type': 'TARGET_SEQUENCE'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_data['projects'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Grab some summary statistics of the modelling.\n",
    "# I'm curious to know how many times a particular template was chosen.\n",
    "\n",
    "template_counter = Counter()\n",
    "gmqe_scores = []\n",
    "sequence_identity = []\n",
    "sequence_similarity = []\n",
    "for p in model_data['projects']:\n",
    "    for m in p['models']:\n",
    "        template_counter[m['template']] += 1\n",
    "        gmqe_scores.append(m['GMQE'])\n",
    "        sequence_identity.append(m['seq_id'])\n",
    "        sequence_similarity.append(m['seq_sim'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  2.00000000e+00,   0.00000000e+00,   1.00000000e+01,\n",
       "          1.60000000e+01,   7.40000000e+01,   2.77000000e+02,\n",
       "          1.15600000e+03,   1.06300000e+03,   2.13600000e+03,\n",
       "          5.46600000e+03]),\n",
       " array([ 0.92199999,  0.92879999,  0.93559999,  0.9424    ,  0.9492    ,\n",
       "         0.956     ,  0.9628    ,  0.9696    ,  0.97640001,  0.98320001,\n",
       "         0.99000001]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEACAYAAABcXmojAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nbz0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb52ba6c390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(gmqe_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   31.,    94.,   303.,  1017.,  1114.,  1457.,  2923.,  2059.,\n",
       "          959.,   243.]),\n",
       " array([  76.76767731,   79.09090958,   81.41414185,   83.73737411,\n",
       "          86.06060638,   88.38383865,   90.70707092,   93.03030319,\n",
       "          95.35353546,   97.67676773,  100.        ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEACAYAAACznAEdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIEiSAANBktQYCJIkwECQJDUGgiQJgP8HUYfEPjpxYx8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb524e30390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(sequence_identity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([    5.,    31.,   141.,   351.,   739.,  1263.,  1859.,  3049.,\n",
       "         2326.,   436.]),\n",
       " array([ 0.5494988 ,  0.55562665,  0.5617545 ,  0.56788235,  0.57401021,\n",
       "         0.58013806,  0.58626591,  0.59239376,  0.59852161,  0.60464947,\n",
       "         0.61077732]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEACAYAAABcXmojAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb522599cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(sequence_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'code': 'wJ9TDy',\n",
       " 'created': '2016-01-02',\n",
       " 'models': [{'GMQE': 0.9800000191,\n",
       "   'id': '01',\n",
       "   'oligo_state': 'homo-dimer',\n",
       "   'qmean': 0.3202740713,\n",
       "   'seq_coverage': 1.0,\n",
       "   'seq_id': 91.9191894531,\n",
       "   'seq_sim': 0.5824936032,\n",
       "   'status': 'COMPLETED',\n",
       "   'template': '4ll3.1.B',\n",
       "   'tpl_seq': 'PQITLWQRPLVTIKIGGQLKEALLDTGADDTVLEEMNLPGRWKPKMIGGIGGFIKVRQYDQILIEICGHKAIGTVLVGPTPVNIIGRNLLTQIGCTLNF',\n",
       "   'trg_seq': 'PQITLWQRPLVTIKIEGQLKEALLDTGADDTVLEEINLSGKWKPKMIGGIGGFIKVGQYDQITIEICGHKVIGTVLVGPTPVNIIGRNLLTQLGCTLNF'},\n",
       "  {'GMQE': 0.9800000191,\n",
       "   'id': '02',\n",
       "   'oligo_state': 'homo-dimer',\n",
       "   'qmean': -0.9170268789,\n",
       "   'seq_coverage': 1.0,\n",
       "   'seq_id': 91.9191894531,\n",
       "   'seq_sim': 0.5824936032,\n",
       "   'status': 'COMPLETED',\n",
       "   'template': '1a8g.1.A',\n",
       "   'tpl_seq': 'PQITLWQRPLVTIKIGGQLKEALLDTGADDTVLEEMNLPGRWKPKMIGGIGGFIKVRQYDQILIEICGHKAIGTVLVGPTPVNIIGRNLLTQIGCTLNF',\n",
       "   'trg_seq': 'PQITLWQRPLVTIKIEGQLKEALLDTGADDTVLEEINLSGKWKPKMIGGIGGFIKVGQYDQITIEICGHKVIGTVLVGPTPVNIIGRNLLTQLGCTLNF'},\n",
       "  {'GMQE': 0.9800000191,\n",
       "   'id': '03',\n",
       "   'oligo_state': 'homo-dimer',\n",
       "   'qmean': -0.5994937920000001,\n",
       "   'seq_coverage': 1.0,\n",
       "   'seq_id': 91.9191894531,\n",
       "   'seq_sim': 0.5824936032,\n",
       "   'status': 'COMPLETED',\n",
       "   'template': '1vik.1.A',\n",
       "   'tpl_seq': 'PQITLWQRPLVTIKIGGQLKEALLDTGADDTVLEEMNLPGRWKPKMIGGIGGFIKVRQYDQILIEICGHKAIGTVLVGPTPVNIIGRNLLTQIGCTLNF',\n",
       "   'trg_seq': 'PQITLWQRPLVTIKIEGQLKEALLDTGADDTVLEEINLSGKWKPKMIGGIGGFIKVGQYDQITIEICGHKVIGTVLVGPTPVNIIGRNLLTQLGCTLNF'}],\n",
       " 'target': 'PQITLWQRPLVTIKIEGQLKEALLDTGADDTVLEEINLSGKWKPKMIGGIGGFIKVGQYDQITIEICGHKVIGTVLVGPTPVNIIGRNLLTQLGCTLNF',\n",
       " 'title': '259265-1',\n",
       " 'type': 'TARGET_SEQUENCE'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_data['projects'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "import networkx as nx\n",
      "import pickle\n",
      "from pin.pin import ProteinInteractionNetwork\n",
      "\n",
      "p = ProteinInteractionNetwork(\"model_01.pdb\")\n",
      "with open(\"model_01.pkl\", \"wb\") as f:\n",
      "    pickle.dump(p, f)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Write the python script here\n",
    "script = '\\n\\\n",
    "import networkx as nx\\n\\\n",
    "import pickle\\n\\\n",
    "from pin.pin import ProteinInteractionNetwork\\n\\\n",
    "\\n\\\n",
    "p = ProteinInteractionNetwork(\"model_01.pdb\")\\n\\\n",
    "with open(\"model_01.pkl\", \"wb\") as f:\\n\\\n",
    "    pickle.dump(p, f)\\n\\\n",
    "'\n",
    "print(script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#!/bin/sh \n",
      "#$ -S /bin/sh \n",
      "#$ -cwd \n",
      "#$ -V \n",
      "#$ -m e \n",
      "#$ -M ericmjl@mit.edu \n",
      "#$ -pe whole_nodes 1 \n",
      "#############################################\n",
      "\n",
      "python script.py\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Write the bash script here\n",
    "bash = '\\n\\\n",
    "#!/bin/sh \\n\\\n",
    "#$ -S /bin/sh \\n\\\n",
    "#$ -cwd \\n\\\n",
    "#$ -V \\n\\\n",
    "#$ -m e \\n\\\n",
    "#$ -M ericmjl@mit.edu \\n\\\n",
    "#$ -pe whole_nodes 1 \\n\\\n",
    "#############################################\\n\\\n",
    "\\n\\\n",
    "python script.py\\n\\\n",
    "'\n",
    "\n",
    "print(bash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#!/bin/sh \n",
      "#$ -S /bin/sh \n",
      "#$ -cwd \n",
      "#$ -V \n",
      "#$ -m e \n",
      "#$ -M ericmjl@mit.edu \n",
      "#$ -pe whole_nodes 1 \n",
      "#############################################\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Write a master script for q-subbing\n",
    "qsub = '\\n\\\n",
    "#!/bin/sh \\n\\\n",
    "#$ -S /bin/sh \\n\\\n",
    "#$ -cwd \\n\\\n",
    "#$ -V \\n\\\n",
    "#$ -m e \\n\\\n",
    "#$ -M ericmjl@mit.edu \\n\\\n",
    "#$ -pe whole_nodes 1 \\n\\\n",
    "#############################################\\n\\\n",
    "\\n\\\n",
    "'\n",
    "\n",
    "print(qsub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Write script to disk\n",
    "mdl_dir = '../data/batch_models/'\n",
    "for project in model_data['projects']:\n",
    "    code = project['code']\n",
    "    with open('{0}/{1}/script.py'.format(mdl_dir, code), 'w+') as f:\n",
    "        f.write(script)\n",
    "    with open('{0}/{1}/{1}.sh'.format(mdl_dir, code), 'w+') as f:\n",
    "        f.write(bash)\n",
    "        \n",
    "\n",
    "with open('{0}/master.sh'.format(mdl_dir), 'w+') as f:\n",
    "    f.write(qsub)\n",
    "    for project in model_data['projects']:\n",
    "        \n",
    "        f.write('cd {0}\\n'.format(project['code']))\n",
    "        f.write('qsub {0}.sh\\n'.format(project['code']))\n",
    "        f.write('cd ..\\n')\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prototype script for parallelizing making graphs and doing deep learning on them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ATV</th>\n",
       "      <th>DRV</th>\n",
       "      <th>FPV</th>\n",
       "      <th>IDV</th>\n",
       "      <th>LPV</th>\n",
       "      <th>NFV</th>\n",
       "      <th>SQV</th>\n",
       "      <th>SeqID</th>\n",
       "      <th>TPV</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5051.000000</td>\n",
       "      <td>2838.000000</td>\n",
       "      <td>6660.000000</td>\n",
       "      <td>6654.000000</td>\n",
       "      <td>6049.000000</td>\n",
       "      <td>6868.000000</td>\n",
       "      <td>6663.000000</td>\n",
       "      <td>6979.000000</td>\n",
       "      <td>4329.000000</td>\n",
       "      <td>6979.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>60.787349</td>\n",
       "      <td>22.294856</td>\n",
       "      <td>23.945511</td>\n",
       "      <td>29.115104</td>\n",
       "      <td>55.100380</td>\n",
       "      <td>49.808736</td>\n",
       "      <td>77.044349</td>\n",
       "      <td>110069.076659</td>\n",
       "      <td>9.378517</td>\n",
       "      <td>0.249033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>138.450445</td>\n",
       "      <td>77.386476</td>\n",
       "      <td>58.589808</td>\n",
       "      <td>65.169955</td>\n",
       "      <td>99.343837</td>\n",
       "      <td>105.497697</td>\n",
       "      <td>214.590607</td>\n",
       "      <td>61028.133923</td>\n",
       "      <td>38.791570</td>\n",
       "      <td>0.316821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>2996.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.020833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>64725.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.031250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.400000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.100000</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>7.800000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>112389.000000</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>46.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>30.150000</td>\n",
       "      <td>143350.000000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>700.000000</td>\n",
       "      <td>580.000000</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>600.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>259265.000000</td>\n",
       "      <td>800.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               ATV          DRV          FPV          IDV          LPV  \\\n",
       "count  5051.000000  2838.000000  6660.000000  6654.000000  6049.000000   \n",
       "mean     60.787349    22.294856    23.945511    29.115104    55.100380   \n",
       "std     138.450445    77.386476    58.589808    65.169955    99.343837   \n",
       "min       0.300000     0.200000     0.100000     0.100000     0.100000   \n",
       "25%       1.000000     0.700000     0.800000     0.900000     0.900000   \n",
       "50%       7.400000     1.000000     2.100000     4.400000     7.800000   \n",
       "75%      46.000000     7.000000    16.000000    28.000000    74.000000   \n",
       "max     700.000000   580.000000   400.000000   500.000000   500.000000   \n",
       "\n",
       "               NFV          SQV          SeqID          TPV       weight  \n",
       "count  6868.000000  6663.000000    6979.000000  4329.000000  6979.000000  \n",
       "mean     49.808736    77.044349  110069.076659     9.378517     0.249033  \n",
       "std     105.497697   214.590607   61028.133923    38.791570     0.316821  \n",
       "min       0.100000     0.100000    2996.000000     0.200000     0.020833  \n",
       "25%       1.300000     0.800000   64725.000000     0.800000     0.031250  \n",
       "50%      10.000000     2.500000  112389.000000     1.300000     0.125000  \n",
       "75%      53.000000    30.150000  143350.000000     3.500000     0.250000  \n",
       "max     600.000000  1000.000000  259265.000000   800.000000     1.000000  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "protease_data = pd.read_csv('../data/hiv_data/hiv-protease-data-expanded.csv', index_col=0)\n",
    "protease_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the number of samples available for each drug, let us start with NFV, which has the most number of values available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    6868.000000\n",
       "mean       49.808736\n",
       "std       105.497697\n",
       "min         0.100000\n",
       "25%         1.300000\n",
       "50%        10.000000\n",
       "75%        53.000000\n",
       "max       600.000000\n",
       "Name: NFV, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "protease_data['NFV'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will log-transform these values, and then Z-standardize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FPV</th>\n",
       "      <th>seqid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.397940</td>\n",
       "      <td>2996-0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.397940</td>\n",
       "      <td>2996-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.154902</td>\n",
       "      <td>4387-0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.154902</td>\n",
       "      <td>4387-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.154902</td>\n",
       "      <td>4387-2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        FPV   seqid\n",
       "0  0.397940  2996-0\n",
       "1  0.397940  2996-1\n",
       "2 -0.154902  4387-0\n",
       "3 -0.154902  4387-1\n",
       "4 -0.154902  4387-2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niPNCYGY24rwQmJmNuP8PJeWQAYRCcfgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11b4f3c88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sklearn.preprocessing as pp\n",
    "# from sklearn.preprocessing import scale\n",
    "import numpy as np\n",
    "\n",
    "# Drop NaN values on only the NFV column\n",
    "fpv_data = protease_data.dropna(subset=['FPV'])[['FPV', 'seqid']]\n",
    "fpv_data['FPV'] = fpv_data['FPV'].apply(np.log10)\n",
    "# fpv_data['FPV_scaled'] = pp.scale(fpv_data['FPV'])\n",
    "fpv_data.hist()\n",
    "fpv_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'102798-2': 'zB9kN6',\n",
       " '112387-1': 'a8TXLz',\n",
       " '61139-1': 'waZQPE',\n",
       " '66668-0': 'jB4TGE'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grab out a sample of projects, based on the seqid.\n",
    "import random as rnd\n",
    "# seqids_interest = rnd.sample(list(fpv_data['seqid']), 25)\n",
    "\n",
    "proj_titles = {c['title']:c['code'] for c in model_data['projects']}\n",
    "\n",
    "\n",
    "projs_interest = dict()\n",
    "\n",
    "n_graphs = 4\n",
    "\n",
    "while len(projs_interest) < n_graphs:\n",
    "    seqid = rnd.choice(list(fpv_data['seqid']))\n",
    "    if seqid in proj_titles.keys():\n",
    "        projs_interest[seqid] = proj_titles[seqid]\n",
    "        \n",
    "        \n",
    "        \n",
    "# for seqid in :\n",
    "#     if seqid not in proj_titles:\n",
    "#         print(seqid)\n",
    "# projects_interest = {c['title']:c['code'] for c in model_data['projects'] if c['title'] in seqids_interest}\n",
    "projs_interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FPV</th>\n",
       "      <th>seqid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1639</th>\n",
       "      <td>1.397940</td>\n",
       "      <td>61139-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1954</th>\n",
       "      <td>0.255273</td>\n",
       "      <td>66668-0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3093</th>\n",
       "      <td>0.973128</td>\n",
       "      <td>102798-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3464</th>\n",
       "      <td>0.939519</td>\n",
       "      <td>112387-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           FPV     seqid\n",
       "1639  1.397940   61139-1\n",
       "1954  0.255273   66668-0\n",
       "3093  0.973128  102798-2\n",
       "3464  0.939519  112387-1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpv_data[fpv_data['seqid'].isin(projs_interest.keys())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "projects = [i for i in projs_interest.values()]\n",
    "\n",
    "p = pin.ProteinInteractionNetwork('../data/batch_models/{0}/model_01.pdb'.format(projects[0]))\n",
    "p.graph['project'] = project\n",
    "p.graph['input_shape'] = p.nodes(data=True)[0][1]['features'].shape\n",
    "p.graph['seqid'] = seqid\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Takes about 2 hours for the task to finish over 3401 graphs. This is it being done on a single core.\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "graphs = []\n",
    "\n",
    "def make_protein_graphs(project, seqid):\n",
    "    \"\"\"\n",
    "    Custom function for this notebook to parallelize the making of protein graphs over individual cores.\n",
    "    \"\"\"\n",
    "    from pin import pin\n",
    "    p = pin.ProteinInteractionNetwork('../data/batch_models/{0}/model_01.pdb'.format(project))\n",
    "    p.graph['project'] = project\n",
    "    p.graph['input_shape'] = p.nodes(data=True)[0][1]['features'].shape\n",
    "    p.graph['seqid'] = seqid\n",
    "    return p\n",
    "\n",
    "graphs = Parallel(n_jobs=-1)(delayed(make_protein_graphs)(proj, seqid) for seqid, proj in projs_interest.items())\n",
    "len(graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 36)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check a sample of the graphs to make sure that the input shapes are correct.\n",
    "graphs[0].graph['input_shape']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from graphfp.layers import FingerprintLayer, GraphInputLayer, LinearRegressionLayer, GraphConvLayer,\\\n",
    "    FullyConnectedLayer, DropoutLayer\n",
    "from graphfp.utils import initialize_network, batch_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 36)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graphs[0].nodes(data=True)[0][1]['features'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_loss(wb_vect, unflattener, batch=True, batch_size=2, debug=False):\n",
    "    \"\"\"\n",
    "    Training loss is MSE.\n",
    "\n",
    "    We pass in a flattened parameter vector and its unflattener.\n",
    "    \"\"\"\n",
    "    wb_struct = unflattener(wb_vect)\n",
    "\n",
    "    if batch:\n",
    "        batch_size = batch_size\n",
    "    else:\n",
    "        batch_size = len(graphs)\n",
    "\n",
    "    samp_graphs, samp_inputs = batch_sample(graphs, input_shape, batch_size)\n",
    "\n",
    "    preds = predict(wb_struct, samp_inputs, samp_graphs)\n",
    "    graph_ids = [g.graph['seqid'] for g in samp_graphs]\n",
    "    # graph_scores = fpv_data[fpv_data['seqid'].isin(graph_ids)]['FPV'].values.reshape(preds.shape)\n",
    "    graph_scores = fpv_data.set_index('seqid').ix[graph_ids]['FPV'].values.reshape(preds.shape)\n",
    "    assert preds.shape == graph_scores.shape\n",
    "    \n",
    "    if debug:\n",
    "        print(graph_ids)\n",
    "        print('Predictions:')\n",
    "        print(preds)\n",
    "        print('Mean: {0}'.format(np.mean(preds)))\n",
    "        print('')\n",
    "        print('Actual')\n",
    "        print(graph_scores)\n",
    "        print('Mean: {0}'.format(np.mean(graph_scores)))\n",
    "        print('')\n",
    "        print('Difference')\n",
    "        print(preds - graph_scores)\n",
    "        print('Mean Squared Error: {0}'.format(np.mean(np.power(preds - graph_scores, 2))))\n",
    "        print('')\n",
    "    # print(preds.shape, graph_scores.shape)\n",
    "    # assert preds.shape[1] == graph_scores.shape[1]\n",
    "    # print(preds - graph_scores)\n",
    "    mse = np.mean(np.power(preds - graph_scores, 2))\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.15490196],\n",
       "       [ 0.39794001],\n",
       "       [ 0.39794001],\n",
       "       [ 0.59106461]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fpv_data[fpv_data['seqid'].isin(['2996-1', '2996-0'])]\n",
    "graph_ids = ['4387-1', '2996-1', '2996-0', '4482-1']\n",
    "fpv_data.set_index('seqid').ix[graph_ids]['FPV'].values.reshape((4,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(wb_struct, inputs, graphs):\n",
    "    \"\"\"\n",
    "    Makes predictions by running the forward pass over all of the layers.\n",
    "\n",
    "    Parameters:\n",
    "    ===========\n",
    "    - wb_struct: a dictionary of weights and biases stored for each layer.\n",
    "    - inputs: the input data matrix. should be one row per graph.\n",
    "    - graphs: a list of all graphs.\n",
    "    \"\"\"\n",
    "    curr_inputs = inputs\n",
    "\n",
    "    for i, layer in enumerate(layers):\n",
    "        # print(type(wb_struct))\n",
    "        wb = wb_struct['layer{0}_{1}'.format(i, layer)]\n",
    "        curr_inputs = layer.forward_pass(wb, curr_inputs, graphs)\n",
    "    return curr_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def callback(wb, i):\n",
    "    \"\"\"\n",
    "    Any function you want to run at each iteration of the optimization.\n",
    "    \"\"\"\n",
    "    # from time import time\n",
    "    from graphfp.flatten import flatten\n",
    "\n",
    "    # new_time = time()\n",
    "    wb_vect, wb_unflattener = flatten(wb)\n",
    "    print('Iteration: {0}'.format(i))\n",
    "    # print('Computing gradient w.r.t. weights...')\n",
    "\n",
    "    print('Training Loss: ')\n",
    "    \n",
    "    tl = train_loss(wb_vect, wb_unflattener, batch=False)\n",
    "    print(tl)\n",
    "    print('')\n",
    "\n",
    "    training_losses.append(tl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from autograd import grad\n",
    "grad_tl = grad(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(graphs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 2,\n",
       "         1: 9,\n",
       "         2: 69,\n",
       "         3: 42,\n",
       "         4: 12,\n",
       "         5: 14,\n",
       "         6: 16,\n",
       "         7: 12,\n",
       "         8: 9,\n",
       "         9: 5,\n",
       "         10: 4,\n",
       "         11: 4})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import networkx as nx\n",
    "from collections import Counter\n",
    "Counter([len(graphs[0].neighbors(n)) for n in graphs[0].nodes()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from graphfp.optimizers import sgd, adam\n",
    "\n",
    "input_shape = graphs[0].graph['input_shape']\n",
    "layers = [# GraphConvLayer((input_shape[1], input_shape[1] * 2)),\n",
    "          # GraphConvLayer((input_shape[1], input_shape[1])),\n",
    "          # GraphConvLayer((input_shape[1], input_shape[1])),\n",
    "          GraphConvLayer((input_shape[1], input_shape[1])),\n",
    "          FingerprintLayer(input_shape[1]),\n",
    "          # FullyConnectedLayer((input_shape[1], input_shape[1])),\n",
    "          # DropoutLayer(p=0),\n",
    "          # FullyConnectedLayer((input_shape[1], input_shape[1])),\n",
    "          # DropoutLayer(p=0.3),\n",
    "          # FullyConnectedLayer((input_shape[1], input_shape[1])),\n",
    "          LinearRegressionLayer((input_shape[1], 1)),\n",
    "]\n",
    "\n",
    "wb_all = initialize_network(input_shape, graphs, layers)\n",
    "\n",
    "training_losses = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['61139-1', '66668-0']\n",
      "Predictions:\n",
      "[[ 0.00202669]\n",
      " [ 0.00200087]]\n",
      "Mean: 0.0020137820125656902\n",
      "\n",
      "Actual\n",
      "[[ 1.39794001]\n",
      " [ 0.25527251]]\n",
      "Mean: 0.8266062568876719\n",
      "\n",
      "Difference\n",
      "[[-1.39591332]\n",
      " [-0.25327163]]\n",
      "Mean Squared Error: 1.0063602570175394\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0063602570175394"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from graphfp.flatten import flatten\n",
    "wb_vect, wb_unflattener = flatten(wb_all)\n",
    "train_loss(wb_vect, wb_unflattener, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "Training Loss: \n",
      "0.958683847731\n",
      "\n",
      "Iteration: 1\n",
      "Training Loss: \n",
      "0.85574625148\n",
      "\n",
      "Iteration: 2\n",
      "Training Loss: \n",
      "0.692849730205\n",
      "\n",
      "Iteration: 3\n",
      "Training Loss: \n",
      "0.470798874929\n",
      "\n",
      "Iteration: 4\n",
      "Training Loss: \n",
      "0.25610279\n",
      "\n",
      "Iteration: 5\n",
      "Training Loss: \n",
      "0.142365458042\n",
      "\n",
      "Iteration: 6\n",
      "Training Loss: \n",
      "0.213195179494\n",
      "\n",
      "Iteration: 7\n",
      "Training Loss: \n",
      "0.247360795651\n",
      "\n",
      "Iteration: 8\n",
      "Training Loss: \n",
      "0.194211675821\n",
      "\n",
      "Iteration: 9\n",
      "Training Loss: \n",
      "0.141595420563\n",
      "\n",
      "Iteration: 10\n",
      "Training Loss: \n",
      "0.142453463102\n",
      "\n",
      "Iteration: 11\n",
      "Training Loss: \n",
      "0.182790006674\n",
      "\n",
      "Iteration: 12\n",
      "Training Loss: \n",
      "0.224292844583\n",
      "\n",
      "Iteration: 13\n",
      "Training Loss: \n",
      "0.252211207652\n",
      "\n",
      "Iteration: 14\n",
      "Training Loss: \n",
      "0.275144920839\n",
      "\n",
      "Iteration: 15\n",
      "Training Loss: \n",
      "0.271139273145\n",
      "\n",
      "Iteration: 16\n",
      "Training Loss: \n",
      "0.260295271967\n",
      "\n",
      "Iteration: 17\n",
      "Training Loss: \n",
      "0.232559707405\n",
      "\n",
      "Iteration: 18\n",
      "Training Loss: \n",
      "0.183562419011\n",
      "\n",
      "Iteration: 19\n",
      "Training Loss: \n",
      "0.136512923223\n",
      "\n",
      "Iteration: 20\n",
      "Training Loss: \n",
      "0.119878363623\n",
      "\n",
      "Iteration: 21\n",
      "Training Loss: \n",
      "0.13110443196\n",
      "\n",
      "Iteration: 22\n",
      "Training Loss: \n",
      "0.151622095818\n",
      "\n",
      "Iteration: 23\n",
      "Training Loss: \n",
      "0.184779148901\n",
      "\n",
      "Iteration: 24\n",
      "Training Loss: \n",
      "0.174512189282\n",
      "\n",
      "Iteration: 25\n",
      "Training Loss: \n",
      "0.152347698212\n",
      "\n",
      "Iteration: 26\n",
      "Training Loss: \n",
      "0.129961992086\n",
      "\n",
      "Iteration: 27\n",
      "Training Loss: \n",
      "0.112996493168\n",
      "\n",
      "Iteration: 28\n",
      "Training Loss: \n",
      "0.112750886689\n",
      "\n",
      "Iteration: 29\n",
      "Training Loss: \n",
      "0.124539538782\n",
      "\n",
      "Iteration: 30\n",
      "Training Loss: \n",
      "0.141850640487\n",
      "\n",
      "Iteration: 31\n",
      "Training Loss: \n",
      "0.146933121219\n",
      "\n",
      "Iteration: 32\n",
      "Training Loss: \n",
      "0.145651126385\n",
      "\n",
      "Iteration: 33\n",
      "Training Loss: \n",
      "0.138544464627\n",
      "\n",
      "Iteration: 34\n",
      "Training Loss: \n",
      "0.134354561001\n",
      "\n",
      "Iteration: 35\n",
      "Training Loss: \n",
      "0.134133521661\n",
      "\n",
      "Iteration: 36\n",
      "Training Loss: \n",
      "0.136118127282\n",
      "\n",
      "Iteration: 37\n",
      "Training Loss: \n",
      "0.141611066827\n",
      "\n",
      "Iteration: 38\n",
      "Training Loss: \n",
      "0.13011848886\n",
      "\n",
      "Iteration: 39\n",
      "Training Loss: \n",
      "0.109799346581\n",
      "\n",
      "Iteration: 40\n",
      "Training Loss: \n",
      "0.0965045666464\n",
      "\n",
      "Iteration: 41\n",
      "Training Loss: \n",
      "0.0975239677667\n",
      "\n",
      "Iteration: 42\n",
      "Training Loss: \n",
      "0.104514083385\n",
      "\n",
      "Iteration: 43\n",
      "Training Loss: \n",
      "0.112808357716\n",
      "\n",
      "Iteration: 44\n",
      "Training Loss: \n",
      "0.115869471847\n",
      "\n",
      "Iteration: 45\n",
      "Training Loss: \n",
      "0.124810155618\n",
      "\n",
      "Iteration: 46\n",
      "Training Loss: \n",
      "0.113135138663\n",
      "\n",
      "Iteration: 47\n",
      "Training Loss: \n",
      "0.0993504632296\n",
      "\n",
      "Iteration: 48\n",
      "Training Loss: \n",
      "0.0879375191262\n",
      "\n",
      "Iteration: 49\n",
      "Training Loss: \n",
      "0.0892524242474\n",
      "\n",
      "Iteration: 50\n",
      "Training Loss: \n",
      "0.104753202377\n",
      "\n",
      "Iteration: 51\n",
      "Training Loss: \n",
      "0.114597812245\n",
      "\n",
      "Iteration: 52\n",
      "Training Loss: \n",
      "0.110250212139\n",
      "\n",
      "Iteration: 53\n",
      "Training Loss: \n",
      "0.101382343703\n",
      "\n",
      "Iteration: 54\n",
      "Training Loss: \n",
      "0.0869070238073\n",
      "\n",
      "Iteration: 55\n",
      "Training Loss: \n",
      "0.0806358964824\n",
      "\n",
      "Iteration: 56\n",
      "Training Loss: \n",
      "0.0834972373044\n",
      "\n",
      "Iteration: 57\n",
      "Training Loss: \n",
      "0.101633686583\n",
      "\n",
      "Iteration: 58\n",
      "Training Loss: \n",
      "0.123440429097\n",
      "\n",
      "Iteration: 59\n",
      "Training Loss: \n",
      "0.155288652636\n",
      "\n",
      "Iteration: 60\n",
      "Training Loss: \n",
      "0.183013727902\n",
      "\n",
      "Iteration: 61\n",
      "Training Loss: \n",
      "0.195235847164\n",
      "\n",
      "Iteration: 62\n",
      "Training Loss: \n",
      "0.168044920568\n",
      "\n",
      "Iteration: 63\n",
      "Training Loss: \n",
      "0.122787689069\n",
      "\n",
      "Iteration: 64\n",
      "Training Loss: \n",
      "0.0861910720916\n",
      "\n",
      "Iteration: 65\n",
      "Training Loss: \n",
      "0.0725613286926\n",
      "\n",
      "Iteration: 66\n",
      "Training Loss: \n",
      "0.0728106354139\n",
      "\n",
      "Iteration: 67\n",
      "Training Loss: \n",
      "0.0941777513554\n",
      "\n",
      "Iteration: 68\n",
      "Training Loss: \n",
      "0.133980906301\n",
      "\n",
      "Iteration: 69\n",
      "Training Loss: \n",
      "0.182764019322\n",
      "\n",
      "Iteration: 70\n",
      "Training Loss: \n",
      "0.214669535146\n",
      "\n",
      "Iteration: 71\n",
      "Training Loss: \n",
      "0.225125808078\n",
      "\n",
      "Iteration: 72\n",
      "Training Loss: \n",
      "0.203719195422\n",
      "\n",
      "Iteration: 73\n",
      "Training Loss: \n",
      "0.180356354256\n",
      "\n",
      "Iteration: 74\n",
      "Training Loss: \n",
      "0.144870701657\n",
      "\n",
      "Iteration: 75\n",
      "Training Loss: \n",
      "0.0985202452612\n",
      "\n",
      "Iteration: 76\n",
      "Training Loss: \n",
      "0.0677800322103\n",
      "\n",
      "Iteration: 77\n",
      "Training Loss: \n",
      "0.0657883428915\n",
      "\n",
      "Iteration: 78\n",
      "Training Loss: \n",
      "0.101011001294\n",
      "\n",
      "Iteration: 79\n",
      "Training Loss: \n",
      "0.125005654418\n",
      "\n",
      "Iteration: 80\n",
      "Training Loss: \n",
      "0.129055485356\n",
      "\n",
      "Iteration: 81\n",
      "Training Loss: \n",
      "0.0958836595543\n",
      "\n",
      "Iteration: 82\n",
      "Training Loss: \n",
      "0.0621714171681\n",
      "\n",
      "Iteration: 83\n",
      "Training Loss: \n",
      "0.0561862660318\n",
      "\n",
      "Iteration: 84\n",
      "Training Loss: \n",
      "0.0685702928111\n",
      "\n",
      "Iteration: 85\n",
      "Training Loss: \n",
      "0.0771508703409\n",
      "\n",
      "Iteration: 86\n",
      "Training Loss: \n",
      "0.0759327987509\n",
      "\n",
      "Iteration: 87\n",
      "Training Loss: \n",
      "0.0652621589671\n",
      "\n",
      "Iteration: 88\n",
      "Training Loss: \n",
      "0.0587603750024\n",
      "\n",
      "Iteration: 89\n",
      "Training Loss: \n",
      "0.0528013077109\n",
      "\n",
      "Iteration: 90\n",
      "Training Loss: \n",
      "0.0501854317193\n",
      "\n",
      "Iteration: 91\n",
      "Training Loss: \n",
      "0.0496285628856\n",
      "\n",
      "Iteration: 92\n",
      "Training Loss: \n",
      "0.047534924388\n",
      "\n",
      "Iteration: 93\n",
      "Training Loss: \n",
      "0.0465758615012\n",
      "\n",
      "Iteration: 94\n",
      "Training Loss: \n",
      "0.0456236375779\n",
      "\n",
      "Iteration: 95\n",
      "Training Loss: \n",
      "0.0447549040204\n",
      "\n",
      "Iteration: 96\n",
      "Training Loss: \n",
      "0.0440957709819\n",
      "\n",
      "Iteration: 97\n",
      "Training Loss: \n",
      "0.0465159571073\n",
      "\n",
      "Iteration: 98\n",
      "Training Loss: \n",
      "0.0567051392617\n",
      "\n",
      "Iteration: 99\n",
      "Training Loss: \n",
      "0.0601017316161\n",
      "\n",
      "Iteration: 100\n",
      "Training Loss: \n",
      "0.0514440109647\n",
      "\n",
      "Iteration: 101\n",
      "Training Loss: \n",
      "0.0468484256098\n",
      "\n",
      "Iteration: 102\n",
      "Training Loss: \n",
      "0.040010839715\n",
      "\n",
      "Iteration: 103\n",
      "Training Loss: \n",
      "0.0381732697536\n",
      "\n",
      "Iteration: 104\n",
      "Training Loss: \n",
      "0.0373667845218\n",
      "\n",
      "Iteration: 105\n",
      "Training Loss: \n",
      "0.0364259108286\n",
      "\n",
      "Iteration: 106\n",
      "Training Loss: \n",
      "0.0358295400567\n",
      "\n",
      "Iteration: 107\n",
      "Training Loss: \n",
      "0.0348203836894\n",
      "\n",
      "Iteration: 108\n",
      "Training Loss: \n",
      "0.0339890105642\n",
      "\n",
      "Iteration: 109\n",
      "Training Loss: \n",
      "0.0335671310875\n",
      "\n",
      "Iteration: 110\n",
      "Training Loss: \n",
      "0.0345228646284\n",
      "\n",
      "Iteration: 111\n",
      "Training Loss: \n",
      "0.0361525208841\n",
      "\n",
      "Iteration: 112\n",
      "Training Loss: \n",
      "0.0370452051671\n",
      "\n",
      "Iteration: 113\n",
      "Training Loss: \n",
      "0.0409557774234\n",
      "\n",
      "Iteration: 114\n",
      "Training Loss: \n",
      "0.0431840340024\n",
      "\n",
      "Iteration: 115\n",
      "Training Loss: \n",
      "0.0419811615288\n",
      "\n",
      "Iteration: 116\n",
      "Training Loss: \n",
      "0.0351427260004\n",
      "\n",
      "Iteration: 117\n",
      "Training Loss: \n",
      "0.0282860923645\n",
      "\n",
      "Iteration: 118\n",
      "Training Loss: \n",
      "0.0296284989935\n",
      "\n",
      "Iteration: 119\n",
      "Training Loss: \n",
      "0.0343693422857\n",
      "\n",
      "Iteration: 120\n",
      "Training Loss: \n",
      "0.0389209352819\n",
      "\n",
      "Iteration: 121\n",
      "Training Loss: \n",
      "0.0462614006246\n",
      "\n",
      "Iteration: 122\n",
      "Training Loss: \n",
      "0.0501749940761\n",
      "\n",
      "Iteration: 123\n",
      "Training Loss: \n",
      "0.04960867412\n",
      "\n",
      "Iteration: 124\n",
      "Training Loss: \n",
      "0.0410379241362\n",
      "\n",
      "Iteration: 125\n",
      "Training Loss: \n",
      "0.0350339954203\n",
      "\n",
      "Iteration: 126\n",
      "Training Loss: \n",
      "0.0259711860736\n",
      "\n",
      "Iteration: 127\n",
      "Training Loss: \n",
      "0.02223031449\n",
      "\n",
      "Iteration: 128\n",
      "Training Loss: \n",
      "0.0255181328253\n",
      "\n",
      "Iteration: 129\n",
      "Training Loss: \n",
      "0.0319941335613\n",
      "\n",
      "Iteration: 130\n",
      "Training Loss: \n",
      "0.0289489527269\n",
      "\n",
      "Iteration: 131\n",
      "Training Loss: \n",
      "0.02431868592\n",
      "\n",
      "Iteration: 132\n",
      "Training Loss: \n",
      "0.0215522534164\n",
      "\n",
      "Iteration: 133\n",
      "Training Loss: \n",
      "0.0199955432468\n",
      "\n",
      "Iteration: 134\n",
      "Training Loss: \n",
      "0.0214490281945\n",
      "\n",
      "Iteration: 135\n",
      "Training Loss: \n",
      "0.0222248928053\n",
      "\n",
      "Iteration: 136\n",
      "Training Loss: \n",
      "0.0205073798518\n",
      "\n",
      "Iteration: 137\n",
      "Training Loss: \n",
      "0.018932077803\n",
      "\n",
      "Iteration: 138\n",
      "Training Loss: \n",
      "0.0185542499901\n",
      "\n",
      "Iteration: 139\n",
      "Training Loss: \n",
      "0.0179695847285\n",
      "\n",
      "Iteration: 140\n",
      "Training Loss: \n",
      "0.0183020325996\n",
      "\n",
      "Iteration: 141\n",
      "Training Loss: \n",
      "0.0184487037251\n",
      "\n",
      "Iteration: 142\n",
      "Training Loss: \n",
      "0.0171156821992\n",
      "\n",
      "Iteration: 143\n",
      "Training Loss: \n",
      "0.0160114835067\n",
      "\n",
      "Iteration: 144\n",
      "Training Loss: \n",
      "0.0152699122895\n",
      "\n",
      "Iteration: 145\n",
      "Training Loss: \n",
      "0.0148434643848\n",
      "\n",
      "Iteration: 146\n",
      "Training Loss: \n",
      "0.0147913058034\n",
      "\n",
      "Iteration: 147\n",
      "Training Loss: \n",
      "0.0143645360522\n",
      "\n",
      "Iteration: 148\n",
      "Training Loss: \n",
      "0.0136603361384\n",
      "\n",
      "Iteration: 149\n",
      "Training Loss: \n",
      "0.0135236835491\n",
      "\n",
      "Iteration: 150\n",
      "Training Loss: \n",
      "0.0137747052921\n",
      "\n",
      "Iteration: 151\n",
      "Training Loss: \n",
      "0.0129545775166\n",
      "\n",
      "Iteration: 152\n",
      "Training Loss: \n",
      "0.0123083259052\n",
      "\n",
      "Iteration: 153\n",
      "Training Loss: \n",
      "0.0123095214152\n",
      "\n",
      "Iteration: 154\n",
      "Training Loss: \n",
      "0.0116748347827\n",
      "\n",
      "Iteration: 155\n",
      "Training Loss: \n",
      "0.011282663391\n",
      "\n",
      "Iteration: 156\n",
      "Training Loss: \n",
      "0.0110285181247\n",
      "\n",
      "Iteration: 157\n",
      "Training Loss: \n",
      "0.0106931779896\n",
      "\n",
      "Iteration: 158\n",
      "Training Loss: \n",
      "0.0108420456454\n",
      "\n",
      "Iteration: 159\n",
      "Training Loss: \n",
      "0.0103630170504\n",
      "\n",
      "Iteration: 160\n",
      "Training Loss: \n",
      "0.0105830050222\n",
      "\n",
      "Iteration: 161\n",
      "Training Loss: \n",
      "0.0107661046013\n",
      "\n",
      "Iteration: 162\n",
      "Training Loss: \n",
      "0.0121912027693\n",
      "\n",
      "Iteration: 163\n",
      "Training Loss: \n",
      "0.0156317838156\n",
      "\n",
      "Iteration: 164\n",
      "Training Loss: \n",
      "0.0159397560015\n",
      "\n",
      "Iteration: 165\n",
      "Training Loss: \n",
      "0.0129784223128\n",
      "\n",
      "Iteration: 166\n",
      "Training Loss: \n",
      "0.00931363365715\n",
      "\n",
      "Iteration: 167\n",
      "Training Loss: \n",
      "0.00907487447762\n",
      "\n",
      "Iteration: 168\n",
      "Training Loss: \n",
      "0.0177646213836\n",
      "\n",
      "Iteration: 169\n",
      "Training Loss: \n",
      "0.0277077321788\n",
      "\n",
      "Iteration: 170\n",
      "Training Loss: \n",
      "0.0413404162684\n",
      "\n",
      "Iteration: 171\n",
      "Training Loss: \n",
      "0.0460996239663\n",
      "\n",
      "Iteration: 172\n",
      "Training Loss: \n",
      "0.0411888391147\n",
      "\n",
      "Iteration: 173\n",
      "Training Loss: \n",
      "0.0330139877804\n",
      "\n",
      "Iteration: 174\n",
      "Training Loss: \n",
      "0.0267420208514\n",
      "\n",
      "Iteration: 175\n",
      "Training Loss: \n",
      "0.0184842771693\n",
      "\n",
      "Iteration: 176\n",
      "Training Loss: \n",
      "0.0141637099658\n",
      "\n",
      "Iteration: 177\n",
      "Training Loss: \n",
      "0.0125039892462\n",
      "\n",
      "Iteration: 178\n",
      "Training Loss: \n",
      "0.00849072318197\n",
      "\n",
      "Iteration: 179\n",
      "Training Loss: \n",
      "0.00666263825681\n",
      "\n",
      "Iteration: 180\n",
      "Training Loss: \n",
      "0.0064570677943\n",
      "\n",
      "Iteration: 181\n",
      "Training Loss: \n",
      "0.00643261256447\n",
      "\n",
      "Iteration: 182\n",
      "Training Loss: \n",
      "0.00631447313954\n",
      "\n",
      "Iteration: 183\n",
      "Training Loss: \n",
      "0.00622675968742\n",
      "\n",
      "Iteration: 184\n",
      "Training Loss: \n",
      "0.00617019196297\n",
      "\n",
      "Iteration: 185\n",
      "Training Loss: \n",
      "0.00618164637026\n",
      "\n",
      "Iteration: 186\n",
      "Training Loss: \n",
      "0.00682079466515\n",
      "\n",
      "Iteration: 187\n",
      "Training Loss: \n",
      "0.0064348478196\n",
      "\n",
      "Iteration: 188\n",
      "Training Loss: \n",
      "0.00594503789964\n",
      "\n",
      "Iteration: 189\n",
      "Training Loss: \n",
      "0.00565292995663\n",
      "\n",
      "Iteration: 190\n",
      "Training Loss: \n",
      "0.00609227867058\n",
      "\n",
      "Iteration: 191\n",
      "Training Loss: \n",
      "0.00836349348526\n",
      "\n",
      "Iteration: 192\n",
      "Training Loss: \n",
      "0.014181487671\n",
      "\n",
      "Iteration: 193\n",
      "Training Loss: \n",
      "0.0162639983036\n",
      "\n",
      "Iteration: 194\n",
      "Training Loss: \n",
      "0.0179539013652\n",
      "\n",
      "Iteration: 195\n",
      "Training Loss: \n",
      "0.015777752195\n",
      "\n",
      "Iteration: 196\n",
      "Training Loss: \n",
      "0.0150936379441\n",
      "\n",
      "Iteration: 197\n",
      "Training Loss: \n",
      "0.0142820762512\n",
      "\n",
      "Iteration: 198\n",
      "Training Loss: \n",
      "0.0121960559565\n",
      "\n",
      "Iteration: 199\n",
      "Training Loss: \n",
      "0.0117508868939\n",
      "\n",
      "Iteration: 200\n",
      "Training Loss: \n",
      "0.00824357839502\n",
      "\n",
      "Iteration: 201\n",
      "Training Loss: \n",
      "0.00547846624731\n",
      "\n",
      "Iteration: 202\n",
      "Training Loss: \n",
      "0.00459214251143\n",
      "\n",
      "Iteration: 203\n",
      "Training Loss: \n",
      "0.00492922052642\n",
      "\n",
      "Iteration: 204\n",
      "Training Loss: \n",
      "0.00621973019283\n",
      "\n",
      "Iteration: 205\n",
      "Training Loss: \n",
      "0.00924491814283\n",
      "\n",
      "Iteration: 206\n",
      "Training Loss: \n",
      "0.00938430072019\n",
      "\n",
      "Iteration: 207\n",
      "Training Loss: \n",
      "0.00991822786582\n",
      "\n",
      "Iteration: 208\n",
      "Training Loss: \n",
      "0.00911972278148\n",
      "\n",
      "Iteration: 209\n",
      "Training Loss: \n",
      "0.00628213903498\n",
      "\n",
      "Iteration: 210\n",
      "Training Loss: \n",
      "0.00387762715793\n",
      "\n",
      "Iteration: 211\n",
      "Training Loss: \n",
      "0.00581080226353\n",
      "\n",
      "Iteration: 212\n",
      "Training Loss: \n",
      "0.00996902485388\n",
      "\n",
      "Iteration: 213\n",
      "Training Loss: \n",
      "0.0177432831887\n",
      "\n",
      "Iteration: 214\n",
      "Training Loss: \n",
      "0.0212318185737\n",
      "\n",
      "Iteration: 215\n",
      "Training Loss: \n",
      "0.0229670938737\n",
      "\n",
      "Iteration: 216\n",
      "Training Loss: \n",
      "0.0171614074744\n",
      "\n",
      "Iteration: 217\n",
      "Training Loss: \n",
      "0.0093798821886\n",
      "\n",
      "Iteration: 218\n",
      "Training Loss: \n",
      "0.00372251619042\n",
      "\n",
      "Iteration: 219\n",
      "Training Loss: \n",
      "0.00344814702923\n",
      "\n",
      "Iteration: 220\n",
      "Training Loss: \n",
      "0.00537964224685\n",
      "\n",
      "Iteration: 221\n",
      "Training Loss: \n",
      "0.00799858700762\n",
      "\n",
      "Iteration: 222\n",
      "Training Loss: \n",
      "0.0121254892453\n",
      "\n",
      "Iteration: 223\n",
      "Training Loss: \n",
      "0.0145668159939\n",
      "\n",
      "Iteration: 224\n",
      "Training Loss: \n",
      "0.0167558977375\n",
      "\n",
      "Iteration: 225\n",
      "Training Loss: \n",
      "0.0194557207221\n",
      "\n",
      "Iteration: 226\n",
      "Training Loss: \n",
      "0.0225269191493\n",
      "\n",
      "Iteration: 227\n",
      "Training Loss: \n",
      "0.0246860301734\n",
      "\n",
      "Iteration: 228\n",
      "Training Loss: \n",
      "0.0201617812129\n",
      "\n",
      "Iteration: 229\n",
      "Training Loss: \n",
      "0.0141508172334\n",
      "\n",
      "Iteration: 230\n",
      "Training Loss: \n",
      "0.00954527075825\n",
      "\n",
      "Iteration: 231\n",
      "Training Loss: \n",
      "0.00524907375335\n",
      "\n",
      "Iteration: 232\n",
      "Training Loss: \n",
      "0.00256630376359\n",
      "\n",
      "Iteration: 233\n",
      "Training Loss: \n",
      "0.00413662668416\n",
      "\n",
      "Iteration: 234\n",
      "Training Loss: \n",
      "0.0109597180187\n",
      "\n",
      "Iteration: 235\n",
      "Training Loss: \n",
      "0.018256478759\n",
      "\n",
      "Iteration: 236\n",
      "Training Loss: \n",
      "0.0211808758477\n",
      "\n",
      "Iteration: 237\n",
      "Training Loss: \n",
      "0.0157388854864\n",
      "\n",
      "Iteration: 238\n",
      "Training Loss: \n",
      "0.0127090642034\n",
      "\n",
      "Iteration: 239\n",
      "Training Loss: \n",
      "0.0115008024981\n",
      "\n",
      "Iteration: 240\n",
      "Training Loss: \n",
      "0.00719714315357\n",
      "\n",
      "Iteration: 241\n",
      "Training Loss: \n",
      "0.00472286008764\n",
      "\n",
      "Iteration: 242\n",
      "Training Loss: \n",
      "0.00353657860891\n",
      "\n",
      "Iteration: 243\n",
      "Training Loss: \n",
      "0.00225673588479\n",
      "\n",
      "Iteration: 244\n",
      "Training Loss: \n",
      "0.00219890555021\n",
      "\n",
      "Iteration: 245\n",
      "Training Loss: \n",
      "0.00256828639837\n",
      "\n",
      "Iteration: 246\n",
      "Training Loss: \n",
      "0.00363872999638\n",
      "\n",
      "Iteration: 247\n",
      "Training Loss: \n",
      "0.00380396028406\n",
      "\n",
      "Iteration: 248\n",
      "Training Loss: \n",
      "0.00429231811801\n",
      "\n",
      "Iteration: 249\n",
      "Training Loss: \n",
      "0.00426787656585\n",
      "\n",
      "Iteration: 250\n",
      "Training Loss: \n",
      "0.00483880649855\n",
      "\n",
      "Iteration: 251\n",
      "Training Loss: \n",
      "0.00352907101429\n",
      "\n",
      "Iteration: 252\n",
      "Training Loss: \n",
      "0.00257318691011\n",
      "\n",
      "Iteration: 253\n",
      "Training Loss: \n",
      "0.00235117016546\n",
      "\n",
      "Iteration: 254\n",
      "Training Loss: \n",
      "0.00278035365166\n",
      "\n",
      "Iteration: 255\n",
      "Training Loss: \n",
      "0.00337228567283\n",
      "\n",
      "Iteration: 256\n",
      "Training Loss: \n",
      "0.00369089906441\n",
      "\n",
      "Iteration: 257\n",
      "Training Loss: \n",
      "0.00457015041337\n",
      "\n",
      "Iteration: 258\n",
      "Training Loss: \n",
      "0.00512209163715\n",
      "\n",
      "Iteration: 259\n",
      "Training Loss: \n",
      "0.00703555549093\n",
      "\n",
      "Iteration: 260\n",
      "Training Loss: \n",
      "0.00648938515427\n",
      "\n",
      "Iteration: 261\n",
      "Training Loss: \n",
      "0.0072146104547\n",
      "\n",
      "Iteration: 262\n",
      "Training Loss: \n",
      "0.00694411905816\n",
      "\n",
      "Iteration: 263\n",
      "Training Loss: \n",
      "0.00463311503853\n",
      "\n",
      "Iteration: 264\n",
      "Training Loss: \n",
      "0.00229616360464\n",
      "\n",
      "Iteration: 265\n",
      "Training Loss: \n",
      "0.00182408567369\n",
      "\n",
      "Iteration: 266\n",
      "Training Loss: \n",
      "0.00360513839725\n",
      "\n",
      "Iteration: 267\n",
      "Training Loss: \n",
      "0.00668732581604\n",
      "\n",
      "Iteration: 268\n",
      "Training Loss: \n",
      "0.00702259300222\n",
      "\n",
      "Iteration: 269\n",
      "Training Loss: \n",
      "0.00758420918636\n",
      "\n",
      "Iteration: 270\n",
      "Training Loss: \n",
      "0.00686062090645\n",
      "\n",
      "Iteration: 271\n",
      "Training Loss: \n",
      "0.00383440183876\n",
      "\n",
      "Iteration: 272\n",
      "Training Loss: \n",
      "0.00223515484313\n",
      "\n",
      "Iteration: 273\n",
      "Training Loss: \n",
      "0.00205782735356\n",
      "\n",
      "Iteration: 274\n",
      "Training Loss: \n",
      "0.00209999963729\n",
      "\n",
      "Iteration: 275\n",
      "Training Loss: \n",
      "0.00301583474208\n",
      "\n",
      "Iteration: 276\n",
      "Training Loss: \n",
      "0.00341259348917\n",
      "\n",
      "Iteration: 277\n",
      "Training Loss: \n",
      "0.00364064689254\n",
      "\n",
      "Iteration: 278\n",
      "Training Loss: \n",
      "0.00363991980792\n",
      "\n",
      "Iteration: 279\n",
      "Training Loss: \n",
      "0.00299317684398\n",
      "\n",
      "Iteration: 280\n",
      "Training Loss: \n",
      "0.0033828467957\n",
      "\n",
      "Iteration: 281\n",
      "Training Loss: \n",
      "0.00426962408291\n",
      "\n",
      "Iteration: 282\n",
      "Training Loss: \n",
      "0.00413038831448\n",
      "\n",
      "Iteration: 283\n",
      "Training Loss: \n",
      "0.00308285878876\n",
      "\n",
      "Iteration: 284\n",
      "Training Loss: \n",
      "0.00161492326112\n",
      "\n",
      "Iteration: 285\n",
      "Training Loss: \n",
      "0.000930367852183\n",
      "\n",
      "Iteration: 286\n",
      "Training Loss: \n",
      "0.000797575078627\n",
      "\n",
      "Iteration: 287\n",
      "Training Loss: \n",
      "0.000903452709506\n",
      "\n",
      "Iteration: 288\n",
      "Training Loss: \n",
      "0.00073495632077\n",
      "\n",
      "Iteration: 289\n",
      "Training Loss: \n",
      "0.0010942244691\n",
      "\n",
      "Iteration: 290\n",
      "Training Loss: \n",
      "0.00203378854053\n",
      "\n",
      "Iteration: 291\n",
      "Training Loss: \n",
      "0.00271727183577\n",
      "\n",
      "Iteration: 292\n",
      "Training Loss: \n",
      "0.00325415300134\n",
      "\n",
      "Iteration: 293\n",
      "Training Loss: \n",
      "0.00235264276351\n",
      "\n",
      "Iteration: 294\n",
      "Training Loss: \n",
      "0.00160350167084\n",
      "\n",
      "Iteration: 295\n",
      "Training Loss: \n",
      "0.000735613856377\n",
      "\n",
      "Iteration: 296\n",
      "Training Loss: \n",
      "0.000643416226154\n",
      "\n",
      "Iteration: 297\n",
      "Training Loss: \n",
      "0.000697233499622\n",
      "\n",
      "Iteration: 298\n",
      "Training Loss: \n",
      "0.00132712277099\n",
      "\n",
      "Iteration: 299\n",
      "Training Loss: \n",
      "0.00182072214844\n",
      "\n",
      "Iteration: 300\n",
      "Training Loss: \n",
      "0.00231285028696\n",
      "\n",
      "Iteration: 301\n",
      "Training Loss: \n",
      "0.00201489088457\n",
      "\n",
      "Iteration: 302\n",
      "Training Loss: \n",
      "0.00211913211682\n",
      "\n",
      "Iteration: 303\n",
      "Training Loss: \n",
      "0.00261170774599\n",
      "\n",
      "Iteration: 304\n",
      "Training Loss: \n",
      "0.00246098962944\n",
      "\n",
      "Iteration: 305\n",
      "Training Loss: \n",
      "0.00158467895631\n",
      "\n",
      "Iteration: 306\n",
      "Training Loss: \n",
      "0.00131018718171\n",
      "\n",
      "Iteration: 307\n",
      "Training Loss: \n",
      "0.00088057371228\n",
      "\n",
      "Iteration: 308\n",
      "Training Loss: \n",
      "0.000780611780238\n",
      "\n",
      "Iteration: 309\n",
      "Training Loss: \n",
      "0.000718566638788\n",
      "\n",
      "Iteration: 310\n",
      "Training Loss: \n",
      "0.000876867715062\n",
      "\n",
      "Iteration: 311\n",
      "Training Loss: \n",
      "0.000661584755275\n",
      "\n",
      "Iteration: 312\n",
      "Training Loss: \n",
      "0.000588974543945\n",
      "\n",
      "Iteration: 313\n",
      "Training Loss: \n",
      "0.000594619498433\n",
      "\n",
      "Iteration: 314\n",
      "Training Loss: \n",
      "0.000871274804973\n",
      "\n",
      "Iteration: 315\n",
      "Training Loss: \n",
      "0.000767166203279\n",
      "\n",
      "Iteration: 316\n",
      "Training Loss: \n",
      "0.000438947546358\n",
      "\n",
      "Iteration: 317\n",
      "Training Loss: \n",
      "0.000380326771819\n",
      "\n",
      "Iteration: 318\n",
      "Training Loss: \n",
      "0.000376691435978\n",
      "\n",
      "Iteration: 319\n",
      "Training Loss: \n",
      "0.000439696797772\n",
      "\n",
      "Iteration: 320\n",
      "Training Loss: \n",
      "0.000616407839657\n",
      "\n",
      "Iteration: 321\n",
      "Training Loss: \n",
      "0.00092410210424\n",
      "\n",
      "Iteration: 322\n",
      "Training Loss: \n",
      "0.000686264208409\n",
      "\n",
      "Iteration: 323\n",
      "Training Loss: \n",
      "0.000369725817121\n",
      "\n",
      "Iteration: 324\n",
      "Training Loss: \n",
      "0.000289588229744\n",
      "\n",
      "Iteration: 325\n",
      "Training Loss: \n",
      "0.000447909884031\n",
      "\n",
      "Iteration: 326\n",
      "Training Loss: \n",
      "0.000402572110703\n",
      "\n",
      "Iteration: 327\n",
      "Training Loss: \n",
      "0.000394739666953\n",
      "\n",
      "Iteration: 328\n",
      "Training Loss: \n",
      "0.000469706127208\n",
      "\n",
      "Iteration: 329\n",
      "Training Loss: \n",
      "0.0001980521428\n",
      "\n",
      "Iteration: 330\n",
      "Training Loss: \n",
      "0.00093195216321\n",
      "\n",
      "Iteration: 331\n",
      "Training Loss: \n",
      "0.00459981962558\n",
      "\n",
      "Iteration: 332\n",
      "Training Loss: \n",
      "0.00767579119422\n",
      "\n",
      "Iteration: 333\n",
      "Training Loss: \n",
      "0.00975805695348\n",
      "\n",
      "Iteration: 334\n",
      "Training Loss: \n",
      "0.0086602228824\n",
      "\n",
      "Iteration: 335\n",
      "Training Loss: \n",
      "0.0068531828552\n",
      "\n",
      "Iteration: 336\n",
      "Training Loss: \n",
      "0.00612185395114\n",
      "\n",
      "Iteration: 337\n",
      "Training Loss: \n",
      "0.00387644453589\n",
      "\n",
      "Iteration: 338\n",
      "Training Loss: \n",
      "0.00237354502976\n",
      "\n",
      "Iteration: 339\n",
      "Training Loss: \n",
      "0.00125501308076\n",
      "\n",
      "Iteration: 340\n",
      "Training Loss: \n",
      "0.000457243371484\n",
      "\n",
      "Iteration: 341\n",
      "Training Loss: \n",
      "0.000615985218282\n",
      "\n",
      "Iteration: 342\n",
      "Training Loss: \n",
      "0.00133836974193\n",
      "\n",
      "Iteration: 343\n",
      "Training Loss: \n",
      "0.00196942438303\n",
      "\n",
      "Iteration: 344\n",
      "Training Loss: \n",
      "0.00310750993956\n",
      "\n",
      "Iteration: 345\n",
      "Training Loss: \n",
      "0.00354560888102\n",
      "\n",
      "Iteration: 346\n",
      "Training Loss: \n",
      "0.00404599694872\n",
      "\n",
      "Iteration: 347\n",
      "Training Loss: \n",
      "0.00393466685483\n",
      "\n",
      "Iteration: 348\n",
      "Training Loss: \n",
      "0.00420012761634\n",
      "\n",
      "Iteration: 349\n",
      "Training Loss: \n",
      "0.00354880599449\n",
      "\n",
      "Iteration: 350\n",
      "Training Loss: \n",
      "0.00129307344956\n",
      "\n",
      "Iteration: 351\n",
      "Training Loss: \n",
      "0.000544520650996\n",
      "\n",
      "Iteration: 352\n",
      "Training Loss: \n",
      "0.000508937751604\n",
      "\n",
      "Iteration: 353\n",
      "Training Loss: \n",
      "0.000827124289544\n",
      "\n",
      "Iteration: 354\n",
      "Training Loss: \n",
      "0.000922549866205\n",
      "\n",
      "Iteration: 355\n",
      "Training Loss: \n",
      "0.000668909902468\n",
      "\n",
      "Iteration: 356\n",
      "Training Loss: \n",
      "0.00105547918351\n",
      "\n",
      "Iteration: 357\n",
      "Training Loss: \n",
      "0.00145826183166\n",
      "\n",
      "Iteration: 358\n",
      "Training Loss: \n",
      "0.00107967629217\n",
      "\n",
      "Iteration: 359\n",
      "Training Loss: \n",
      "0.00100340132588\n",
      "\n",
      "Iteration: 360\n",
      "Training Loss: \n",
      "0.00086773980901\n",
      "\n",
      "Iteration: 361\n",
      "Training Loss: \n",
      "0.000334894840434\n",
      "\n",
      "Iteration: 362\n",
      "Training Loss: \n",
      "0.000136235897863\n",
      "\n",
      "Iteration: 363\n",
      "Training Loss: \n",
      "0.000120362785005\n",
      "\n",
      "Iteration: 364\n",
      "Training Loss: \n",
      "0.000203334737144\n",
      "\n",
      "Iteration: 365\n",
      "Training Loss: \n",
      "0.000148267587508\n",
      "\n",
      "Iteration: 366\n",
      "Training Loss: \n",
      "0.000166878207171\n",
      "\n",
      "Iteration: 367\n",
      "Training Loss: \n",
      "0.000176674931482\n",
      "\n",
      "Iteration: 368\n",
      "Training Loss: \n",
      "0.000191593518465\n",
      "\n",
      "Iteration: 369\n",
      "Training Loss: \n",
      "0.000198916895776\n",
      "\n",
      "Iteration: 370\n",
      "Training Loss: \n",
      "0.000234069818729\n",
      "\n",
      "Iteration: 371\n",
      "Training Loss: \n",
      "0.000336284340066\n",
      "\n",
      "Iteration: 372\n",
      "Training Loss: \n",
      "0.000752922122366\n",
      "\n",
      "Iteration: 373\n",
      "Training Loss: \n",
      "0.000717711294576\n",
      "\n",
      "Iteration: 374\n",
      "Training Loss: \n",
      "0.000210660432339\n",
      "\n",
      "Iteration: 375\n",
      "Training Loss: \n",
      "0.000907836311933\n",
      "\n",
      "Iteration: 376\n",
      "Training Loss: \n",
      "0.00480606663776\n",
      "\n",
      "Iteration: 377\n",
      "Training Loss: \n",
      "0.0104013956437\n",
      "\n",
      "Iteration: 378\n",
      "Training Loss: \n",
      "0.0154672629008\n",
      "\n",
      "Iteration: 379\n",
      "Training Loss: \n",
      "0.017637155653\n",
      "\n",
      "Iteration: 380\n",
      "Training Loss: \n",
      "0.0182295777956\n",
      "\n",
      "Iteration: 381\n",
      "Training Loss: \n",
      "0.0130175707689\n",
      "\n",
      "Iteration: 382\n",
      "Training Loss: \n",
      "0.00588724682821\n",
      "\n",
      "Iteration: 383\n",
      "Training Loss: \n",
      "0.00117087513935\n",
      "\n",
      "Iteration: 384\n",
      "Training Loss: \n",
      "0.000718492414904\n",
      "\n",
      "Iteration: 385\n",
      "Training Loss: \n",
      "0.00318698396295\n",
      "\n",
      "Iteration: 386\n",
      "Training Loss: \n",
      "0.00696736424896\n",
      "\n",
      "Iteration: 387\n",
      "Training Loss: \n",
      "0.00748421814372\n",
      "\n",
      "Iteration: 388\n",
      "Training Loss: \n",
      "0.00466669169228\n",
      "\n",
      "Iteration: 389\n",
      "Training Loss: \n",
      "0.00242994618136\n",
      "\n",
      "Iteration: 390\n",
      "Training Loss: \n",
      "0.00147294006991\n",
      "\n",
      "Iteration: 391\n",
      "Training Loss: \n",
      "0.00118612912936\n",
      "\n",
      "Iteration: 392\n",
      "Training Loss: \n",
      "0.00254496345426\n",
      "\n",
      "Iteration: 393\n",
      "Training Loss: \n",
      "0.00636333192274\n",
      "\n",
      "Iteration: 394\n",
      "Training Loss: \n",
      "0.0120049333547\n",
      "\n",
      "Iteration: 395\n",
      "Training Loss: \n",
      "0.0175405740068\n",
      "\n",
      "Iteration: 396\n",
      "Training Loss: \n",
      "0.017836954621\n",
      "\n",
      "Iteration: 397\n",
      "Training Loss: \n",
      "0.0156538849184\n",
      "\n",
      "Iteration: 398\n",
      "Training Loss: \n",
      "0.00986138832065\n",
      "\n",
      "Iteration: 399\n",
      "Training Loss: \n",
      "0.00554426692145\n",
      "\n",
      "Iteration: 400\n",
      "Training Loss: \n",
      "0.00191045068725\n",
      "\n",
      "Iteration: 401\n",
      "Training Loss: \n",
      "0.000990372344912\n",
      "\n",
      "Iteration: 402\n",
      "Training Loss: \n",
      "0.00253705124573\n",
      "\n",
      "Iteration: 403\n",
      "Training Loss: \n",
      "0.00586773641039\n",
      "\n",
      "Iteration: 404\n",
      "Training Loss: \n",
      "0.00710223823354\n",
      "\n",
      "Iteration: 405\n",
      "Training Loss: \n",
      "0.00835233423112\n",
      "\n",
      "Iteration: 406\n",
      "Training Loss: \n",
      "0.00768320661852\n",
      "\n",
      "Iteration: 407\n",
      "Training Loss: \n",
      "0.00715299944146\n",
      "\n",
      "Iteration: 408\n",
      "Training Loss: \n",
      "0.00343624205038\n",
      "\n",
      "Iteration: 409\n",
      "Training Loss: \n",
      "0.00118733808646\n",
      "\n",
      "Iteration: 410\n",
      "Training Loss: \n",
      "0.000386264090601\n",
      "\n",
      "Iteration: 411\n",
      "Training Loss: \n",
      "0.000171619334499\n",
      "\n",
      "Iteration: 412\n",
      "Training Loss: \n",
      "0.000154382688553\n",
      "\n",
      "Iteration: 413\n",
      "Training Loss: \n",
      "0.000272579089603\n",
      "\n",
      "Iteration: 414\n",
      "Training Loss: \n",
      "0.000603252600574\n",
      "\n",
      "Iteration: 415\n",
      "Training Loss: \n",
      "0.00201164786216\n",
      "\n",
      "Iteration: 416\n",
      "Training Loss: \n",
      "0.00542014321206\n",
      "\n",
      "Iteration: 417\n",
      "Training Loss: \n",
      "0.00871257228765\n",
      "\n",
      "Iteration: 418\n",
      "Training Loss: \n",
      "0.0103341153252\n",
      "\n",
      "Iteration: 419\n",
      "Training Loss: \n",
      "0.00784567829209\n",
      "\n",
      "Iteration: 420\n",
      "Training Loss: \n",
      "0.00358597075538\n",
      "\n",
      "Iteration: 421\n",
      "Training Loss: \n",
      "0.000746015708781\n",
      "\n",
      "Iteration: 422\n",
      "Training Loss: \n",
      "0.000316077648235\n",
      "\n",
      "Iteration: 423\n",
      "Training Loss: \n",
      "0.00114988771702\n",
      "\n",
      "Iteration: 424\n",
      "Training Loss: \n",
      "0.003214579569\n",
      "\n",
      "Iteration: 425\n",
      "Training Loss: \n",
      "0.00626009563807\n",
      "\n",
      "Iteration: 426\n",
      "Training Loss: \n",
      "0.00626748035961\n",
      "\n",
      "Iteration: 427\n",
      "Training Loss: \n",
      "0.00395470673718\n",
      "\n",
      "Iteration: 428\n",
      "Training Loss: \n",
      "0.00250997154429\n",
      "\n",
      "Iteration: 429\n",
      "Training Loss: \n",
      "0.00177744400177\n",
      "\n",
      "Iteration: 430\n",
      "Training Loss: \n",
      "0.00122229874832\n",
      "\n",
      "Iteration: 431\n",
      "Training Loss: \n",
      "0.00114501997724\n",
      "\n",
      "Iteration: 432\n",
      "Training Loss: \n",
      "0.00132100643753\n",
      "\n",
      "Iteration: 433\n",
      "Training Loss: \n",
      "0.00159586962378\n",
      "\n",
      "Iteration: 434\n",
      "Training Loss: \n",
      "0.00187480040502\n",
      "\n",
      "Iteration: 435\n",
      "Training Loss: \n",
      "0.00204811241989\n",
      "\n",
      "Iteration: 436\n",
      "Training Loss: \n",
      "0.00259317399578\n",
      "\n",
      "Iteration: 437\n",
      "Training Loss: \n",
      "0.00408086194335\n",
      "\n",
      "Iteration: 438\n",
      "Training Loss: \n",
      "0.00419170136325\n",
      "\n",
      "Iteration: 439\n",
      "Training Loss: \n",
      "0.00500683137313\n",
      "\n",
      "Iteration: 440\n",
      "Training Loss: \n",
      "0.00647919415586\n",
      "\n",
      "Iteration: 441\n",
      "Training Loss: \n",
      "0.00668693680653\n",
      "\n",
      "Iteration: 442\n",
      "Training Loss: \n",
      "0.00615890378266\n",
      "\n",
      "Iteration: 443\n",
      "Training Loss: \n",
      "0.00410972672368\n",
      "\n",
      "Iteration: 444\n",
      "Training Loss: \n",
      "0.00246675458811\n",
      "\n",
      "Iteration: 445\n",
      "Training Loss: \n",
      "0.00192714963909\n",
      "\n",
      "Iteration: 446\n",
      "Training Loss: \n",
      "0.00157932027577\n",
      "\n",
      "Iteration: 447\n",
      "Training Loss: \n",
      "0.00195626170576\n",
      "\n",
      "Iteration: 448\n",
      "Training Loss: \n",
      "0.00136905575078\n",
      "\n",
      "Iteration: 449\n",
      "Training Loss: \n",
      "0.000572472145692\n",
      "\n",
      "Iteration: 450\n",
      "Training Loss: \n",
      "0.000480928139048\n",
      "\n",
      "Iteration: 451\n",
      "Training Loss: \n",
      "0.000435533074488\n",
      "\n",
      "Iteration: 452\n",
      "Training Loss: \n",
      "0.000427956443383\n",
      "\n",
      "Iteration: 453\n",
      "Training Loss: \n",
      "0.000454372460822\n",
      "\n",
      "Iteration: 454\n",
      "Training Loss: \n",
      "0.000995986688886\n",
      "\n",
      "Iteration: 455\n",
      "Training Loss: \n",
      "0.00194534314978\n",
      "\n",
      "Iteration: 456\n",
      "Training Loss: \n",
      "0.00320433153142\n",
      "\n",
      "Iteration: 457\n",
      "Training Loss: \n",
      "0.00360060934751\n",
      "\n",
      "Iteration: 458\n",
      "Training Loss: \n",
      "0.00350119405763\n",
      "\n",
      "Iteration: 459\n",
      "Training Loss: \n",
      "0.00253272916324\n",
      "\n",
      "Iteration: 460\n",
      "Training Loss: \n",
      "0.0017826120026\n",
      "\n",
      "Iteration: 461\n",
      "Training Loss: \n",
      "0.00085724291894\n",
      "\n",
      "Iteration: 462\n",
      "Training Loss: \n",
      "0.000706256727642\n",
      "\n",
      "Iteration: 463\n",
      "Training Loss: \n",
      "0.000741094949164\n",
      "\n",
      "Iteration: 464\n",
      "Training Loss: \n",
      "0.0013468709779\n",
      "\n",
      "Iteration: 465\n",
      "Training Loss: \n",
      "0.00221250379975\n",
      "\n",
      "Iteration: 466\n",
      "Training Loss: \n",
      "0.00208881043299\n",
      "\n",
      "Iteration: 467\n",
      "Training Loss: \n",
      "0.00117803431823\n",
      "\n",
      "Iteration: 468\n",
      "Training Loss: \n",
      "0.000586081537487\n",
      "\n",
      "Iteration: 469\n",
      "Training Loss: \n",
      "0.000176403721173\n",
      "\n",
      "Iteration: 470\n",
      "Training Loss: \n",
      "0.000379364105665\n",
      "\n",
      "Iteration: 471\n",
      "Training Loss: \n",
      "0.00154769375469\n",
      "\n",
      "Iteration: 472\n",
      "Training Loss: \n",
      "0.00219359294218\n",
      "\n",
      "Iteration: 473\n",
      "Training Loss: \n",
      "0.00305310298931\n",
      "\n",
      "Iteration: 474\n",
      "Training Loss: \n",
      "0.00241692562793\n",
      "\n",
      "Iteration: 475\n",
      "Training Loss: \n",
      "0.00134190338243\n",
      "\n",
      "Iteration: 476\n",
      "Training Loss: \n",
      "0.000680072612491\n",
      "\n",
      "Iteration: 477\n",
      "Training Loss: \n",
      "0.000222344338278\n",
      "\n",
      "Iteration: 478\n",
      "Training Loss: \n",
      "0.00074809308139\n",
      "\n",
      "Iteration: 479\n",
      "Training Loss: \n",
      "0.00183824679029\n",
      "\n",
      "Iteration: 480\n",
      "Training Loss: \n",
      "0.00257537160232\n",
      "\n",
      "Iteration: 481\n",
      "Training Loss: \n",
      "0.00303733562417\n",
      "\n",
      "Iteration: 482\n",
      "Training Loss: \n",
      "0.00310453968218\n",
      "\n",
      "Iteration: 483\n",
      "Training Loss: \n",
      "0.00374044006681\n",
      "\n",
      "Iteration: 484\n",
      "Training Loss: \n",
      "0.00331605647459\n",
      "\n",
      "Iteration: 485\n",
      "Training Loss: \n",
      "0.00274012663424\n",
      "\n",
      "Iteration: 486\n",
      "Training Loss: \n",
      "0.00203146359617\n",
      "\n",
      "Iteration: 487\n",
      "Training Loss: \n",
      "0.00201433902896\n",
      "\n",
      "Iteration: 488\n",
      "Training Loss: \n",
      "0.00130801451854\n",
      "\n",
      "Iteration: 489\n",
      "Training Loss: \n",
      "0.00051363635531\n",
      "\n",
      "Iteration: 490\n",
      "Training Loss: \n",
      "0.00024031559362\n",
      "\n",
      "Iteration: 491\n",
      "Training Loss: \n",
      "0.000338425355481\n",
      "\n",
      "Iteration: 492\n",
      "Training Loss: \n",
      "0.00085672654352\n",
      "\n",
      "Iteration: 493\n",
      "Training Loss: \n",
      "0.00191044812575\n",
      "\n",
      "Iteration: 494\n",
      "Training Loss: \n",
      "0.0034672066161\n",
      "\n",
      "Iteration: 495\n",
      "Training Loss: \n",
      "0.00474335489765\n",
      "\n",
      "Iteration: 496\n",
      "Training Loss: \n",
      "0.00572011462254\n",
      "\n",
      "Iteration: 497\n",
      "Training Loss: \n",
      "0.00517555771254\n",
      "\n",
      "Iteration: 498\n",
      "Training Loss: \n",
      "0.00279485746631\n",
      "\n",
      "Iteration: 499\n",
      "Training Loss: \n",
      "0.00134376567533\n",
      "\n",
      "Iteration: 500\n",
      "Training Loss: \n",
      "0.000515691576435\n",
      "\n",
      "Iteration: 501\n",
      "Training Loss: \n",
      "0.000180300841207\n",
      "\n",
      "Iteration: 502\n",
      "Training Loss: \n",
      "0.0001403247192\n",
      "\n",
      "Iteration: 503\n",
      "Training Loss: \n",
      "0.000174808673559\n",
      "\n",
      "Iteration: 504\n",
      "Training Loss: \n",
      "0.000155953116706\n",
      "\n",
      "Iteration: 505\n",
      "Training Loss: \n",
      "0.00014078787781\n",
      "\n",
      "Iteration: 506\n",
      "Training Loss: \n",
      "0.000149403726623\n",
      "\n",
      "Iteration: 507\n",
      "Training Loss: \n",
      "0.000139997124879\n",
      "\n",
      "Iteration: 508\n",
      "Training Loss: \n",
      "0.00020485451795\n",
      "\n",
      "Iteration: 509\n",
      "Training Loss: \n",
      "0.000517116911758\n",
      "\n",
      "Iteration: 510\n",
      "Training Loss: \n",
      "0.000731095282015\n",
      "\n",
      "Iteration: 511\n",
      "Training Loss: \n",
      "0.000657632728755\n",
      "\n",
      "Iteration: 512\n",
      "Training Loss: \n",
      "0.0004156492656\n",
      "\n",
      "Iteration: 513\n",
      "Training Loss: \n",
      "0.000111096730402\n",
      "\n",
      "Iteration: 514\n",
      "Training Loss: \n",
      "6.7558118111e-05\n",
      "\n",
      "Iteration: 515\n",
      "Training Loss: \n",
      "0.000321731787666\n",
      "\n",
      "Iteration: 516\n",
      "Training Loss: \n",
      "0.000445527956482\n",
      "\n",
      "Iteration: 517\n",
      "Training Loss: \n",
      "0.000121022158958\n",
      "\n",
      "Iteration: 518\n",
      "Training Loss: \n",
      "8.70064530168e-05\n",
      "\n",
      "Iteration: 519\n",
      "Training Loss: \n",
      "0.00071267757631\n",
      "\n",
      "Iteration: 520\n",
      "Training Loss: \n",
      "0.00153626196465\n",
      "\n",
      "Iteration: 521\n",
      "Training Loss: \n",
      "0.00261928799793\n",
      "\n",
      "Iteration: 522\n",
      "Training Loss: \n",
      "0.00300381118837\n",
      "\n",
      "Iteration: 523\n",
      "Training Loss: \n",
      "0.00313690922477\n",
      "\n",
      "Iteration: 524\n",
      "Training Loss: \n",
      "0.00377571993173\n",
      "\n",
      "Iteration: 525\n",
      "Training Loss: \n",
      "0.00318411507537\n",
      "\n",
      "Iteration: 526\n",
      "Training Loss: \n",
      "0.00182107493395\n",
      "\n",
      "Iteration: 527\n",
      "Training Loss: \n",
      "0.000645167536915\n",
      "\n",
      "Iteration: 528\n",
      "Training Loss: \n",
      "0.000240737602317\n",
      "\n",
      "Iteration: 529\n",
      "Training Loss: \n",
      "0.000339356370688\n",
      "\n",
      "Iteration: 530\n",
      "Training Loss: \n",
      "0.000285525720697\n",
      "\n",
      "Iteration: 531\n",
      "Training Loss: \n",
      "0.000391767752597\n",
      "\n",
      "Iteration: 532\n",
      "Training Loss: \n",
      "0.000621261977573\n",
      "\n",
      "Iteration: 533\n",
      "Training Loss: \n",
      "0.000690128266552\n",
      "\n",
      "Iteration: 534\n",
      "Training Loss: \n",
      "0.000551011042247\n",
      "\n",
      "Iteration: 535\n",
      "Training Loss: \n",
      "0.000548153223891\n",
      "\n",
      "Iteration: 536\n",
      "Training Loss: \n",
      "0.000665027178528\n",
      "\n",
      "Iteration: 537\n",
      "Training Loss: \n",
      "0.000582992133749\n",
      "\n",
      "Iteration: 538\n",
      "Training Loss: \n",
      "0.000394667864915\n",
      "\n",
      "Iteration: 539\n",
      "Training Loss: \n",
      "0.000328978065771\n",
      "\n",
      "Iteration: 540\n",
      "Training Loss: \n",
      "0.000355293019614\n",
      "\n",
      "Iteration: 541\n",
      "Training Loss: \n",
      "0.000440134122899\n",
      "\n",
      "Iteration: 542\n",
      "Training Loss: \n",
      "0.000459216378824\n",
      "\n",
      "Iteration: 543\n",
      "Training Loss: \n",
      "0.000498600266897\n",
      "\n",
      "Iteration: 544\n",
      "Training Loss: \n",
      "0.000589021834349\n",
      "\n",
      "Iteration: 545\n",
      "Training Loss: \n",
      "0.000684922172024\n",
      "\n",
      "Iteration: 546\n",
      "Training Loss: \n",
      "0.000903998047099\n",
      "\n",
      "Iteration: 547\n",
      "Training Loss: \n",
      "0.000613315945746\n",
      "\n",
      "Iteration: 548\n",
      "Training Loss: \n",
      "0.000400649251133\n",
      "\n",
      "Iteration: 549\n",
      "Training Loss: \n",
      "0.000272788372055\n",
      "\n",
      "Iteration: 550\n",
      "Training Loss: \n",
      "0.000214596264935\n",
      "\n",
      "Iteration: 551\n",
      "Training Loss: \n",
      "0.000232688696401\n",
      "\n",
      "Iteration: 552\n",
      "Training Loss: \n",
      "0.000202708335952\n",
      "\n",
      "Iteration: 553\n",
      "Training Loss: \n",
      "0.000196689436099\n",
      "\n",
      "Iteration: 554\n",
      "Training Loss: \n",
      "0.000210894220444\n",
      "\n",
      "Iteration: 555\n",
      "Training Loss: \n",
      "0.000182684826522\n",
      "\n",
      "Iteration: 556\n",
      "Training Loss: \n",
      "0.000144129480734\n",
      "\n",
      "Iteration: 557\n",
      "Training Loss: \n",
      "0.000132438240099\n",
      "\n",
      "Iteration: 558\n",
      "Training Loss: \n",
      "0.000138353364691\n",
      "\n",
      "Iteration: 559\n",
      "Training Loss: \n",
      "0.000243741072717\n",
      "\n",
      "Iteration: 560\n",
      "Training Loss: \n",
      "0.000231081938886\n",
      "\n",
      "Iteration: 561\n",
      "Training Loss: \n",
      "0.000183828375532\n",
      "\n",
      "Iteration: 562\n",
      "Training Loss: \n",
      "0.000425706727179\n",
      "\n",
      "Iteration: 563\n",
      "Training Loss: \n",
      "0.000840053784596\n",
      "\n",
      "Iteration: 564\n",
      "Training Loss: \n",
      "0.00128489030214\n",
      "\n",
      "Iteration: 565\n",
      "Training Loss: \n",
      "0.00182227262953\n",
      "\n",
      "Iteration: 566\n",
      "Training Loss: \n",
      "0.00233016584607\n",
      "\n",
      "Iteration: 567\n",
      "Training Loss: \n",
      "0.00201679933753\n",
      "\n",
      "Iteration: 568\n",
      "Training Loss: \n",
      "0.00117252799671\n",
      "\n",
      "Iteration: 569\n",
      "Training Loss: \n",
      "0.000658025425881\n",
      "\n",
      "Iteration: 570\n",
      "Training Loss: \n",
      "0.00019087343214\n",
      "\n",
      "Iteration: 571\n",
      "Training Loss: \n",
      "0.000203907430576\n",
      "\n",
      "Iteration: 572\n",
      "Training Loss: \n",
      "0.000950851540583\n",
      "\n",
      "Iteration: 573\n",
      "Training Loss: \n",
      "0.00203314062082\n",
      "\n",
      "Iteration: 574\n",
      "Training Loss: \n",
      "0.00204479835329\n",
      "\n",
      "Iteration: 575\n",
      "Training Loss: \n",
      "0.000830217512726\n",
      "\n",
      "Iteration: 576\n",
      "Training Loss: \n",
      "0.000281358688057\n",
      "\n",
      "Iteration: 577\n",
      "Training Loss: \n",
      "0.000611939550217\n",
      "\n",
      "Iteration: 578\n",
      "Training Loss: \n",
      "0.00188654532317\n",
      "\n",
      "Iteration: 579\n",
      "Training Loss: \n",
      "0.00302031806269\n",
      "\n",
      "Iteration: 580\n",
      "Training Loss: \n",
      "0.00325411100897\n",
      "\n",
      "Iteration: 581\n",
      "Training Loss: \n",
      "0.0025453916656\n",
      "\n",
      "Iteration: 582\n",
      "Training Loss: \n",
      "0.00162741480597\n",
      "\n",
      "Iteration: 583\n",
      "Training Loss: \n",
      "0.000854513170542\n",
      "\n",
      "Iteration: 584\n",
      "Training Loss: \n",
      "0.000317989198881\n",
      "\n",
      "Iteration: 585\n",
      "Training Loss: \n",
      "0.000347085009374\n",
      "\n",
      "Iteration: 586\n",
      "Training Loss: \n",
      "0.000573594601613\n",
      "\n",
      "Iteration: 587\n",
      "Training Loss: \n",
      "0.000387478943275\n",
      "\n",
      "Iteration: 588\n",
      "Training Loss: \n",
      "0.000257209403393\n",
      "\n",
      "Iteration: 589\n",
      "Training Loss: \n",
      "0.000616386505955\n",
      "\n",
      "Iteration: 590\n",
      "Training Loss: \n",
      "0.00113880644161\n",
      "\n",
      "Iteration: 591\n",
      "Training Loss: \n",
      "0.00128041153993\n",
      "\n",
      "Iteration: 592\n",
      "Training Loss: \n",
      "0.00190346814707\n",
      "\n",
      "Iteration: 593\n",
      "Training Loss: \n",
      "0.00184692517054\n",
      "\n",
      "Iteration: 594\n",
      "Training Loss: \n",
      "0.00140516219208\n",
      "\n",
      "Iteration: 595\n",
      "Training Loss: \n",
      "0.00100957923541\n",
      "\n",
      "Iteration: 596\n",
      "Training Loss: \n",
      "0.000467397951724\n",
      "\n",
      "Iteration: 597\n",
      "Training Loss: \n",
      "0.0001582586546\n",
      "\n",
      "Iteration: 598\n",
      "Training Loss: \n",
      "0.00037885702482\n",
      "\n",
      "Iteration: 599\n",
      "Training Loss: \n",
      "0.00104605862238\n",
      "\n",
      "Iteration: 600\n",
      "Training Loss: \n",
      "0.000903783376312\n",
      "\n",
      "Iteration: 601\n",
      "Training Loss: \n",
      "0.000679917298898\n",
      "\n",
      "Iteration: 602\n",
      "Training Loss: \n",
      "0.000368882909822\n",
      "\n",
      "Iteration: 603\n",
      "Training Loss: \n",
      "0.000193500028838\n",
      "\n",
      "Iteration: 604\n",
      "Training Loss: \n",
      "0.00066506257235\n",
      "\n",
      "Iteration: 605\n",
      "Training Loss: \n",
      "0.00123013798448\n",
      "\n",
      "Iteration: 606\n",
      "Training Loss: \n",
      "0.00194676135659\n",
      "\n",
      "Iteration: 607\n",
      "Training Loss: \n",
      "0.00204394617556\n",
      "\n",
      "Iteration: 608\n",
      "Training Loss: \n",
      "0.00175935478908\n",
      "\n",
      "Iteration: 609\n",
      "Training Loss: \n",
      "0.00105798216419\n",
      "\n",
      "Iteration: 610\n",
      "Training Loss: \n",
      "0.000502334874925\n",
      "\n",
      "Iteration: 611\n",
      "Training Loss: \n",
      "0.000215436468102\n",
      "\n",
      "Iteration: 612\n",
      "Training Loss: \n",
      "0.000217239525437\n",
      "\n",
      "Iteration: 613\n",
      "Training Loss: \n",
      "0.000198281443104\n",
      "\n",
      "Iteration: 614\n",
      "Training Loss: \n",
      "0.000186970483171\n",
      "\n",
      "Iteration: 615\n",
      "Training Loss: \n",
      "0.000179159240956\n",
      "\n",
      "Iteration: 616\n",
      "Training Loss: \n",
      "0.000159300914191\n",
      "\n",
      "Iteration: 617\n",
      "Training Loss: \n",
      "0.000142523493978\n",
      "\n",
      "Iteration: 618\n",
      "Training Loss: \n",
      "0.000122076710502\n",
      "\n",
      "Iteration: 619\n",
      "Training Loss: \n",
      "0.000134888359084\n",
      "\n",
      "Iteration: 620\n",
      "Training Loss: \n",
      "0.000120713029485\n",
      "\n",
      "Iteration: 621\n",
      "Training Loss: \n",
      "8.97785602871e-05\n",
      "\n",
      "Iteration: 622\n",
      "Training Loss: \n",
      "7.91219470625e-05\n",
      "\n",
      "Iteration: 623\n",
      "Training Loss: \n",
      "7.23528337585e-05\n",
      "\n",
      "Iteration: 624\n",
      "Training Loss: \n",
      "8.14944293248e-05\n",
      "\n",
      "Iteration: 625\n",
      "Training Loss: \n",
      "7.24262477343e-05\n",
      "\n",
      "Iteration: 626\n",
      "Training Loss: \n",
      "7.11510576224e-05\n",
      "\n",
      "Iteration: 627\n",
      "Training Loss: \n",
      "7.54749315209e-05\n",
      "\n",
      "Iteration: 628\n",
      "Training Loss: \n",
      "0.000100911972488\n",
      "\n",
      "Iteration: 629\n",
      "Training Loss: \n",
      "7.70308644659e-05\n",
      "\n",
      "Iteration: 630\n",
      "Training Loss: \n",
      "7.40180956256e-05\n",
      "\n",
      "Iteration: 631\n",
      "Training Loss: \n",
      "8.90123220255e-05\n",
      "\n",
      "Iteration: 632\n",
      "Training Loss: \n",
      "0.000361975498481\n",
      "\n",
      "Iteration: 633\n",
      "Training Loss: \n",
      "0.000574410405006\n",
      "\n",
      "Iteration: 634\n",
      "Training Loss: \n",
      "0.000648080329279\n",
      "\n",
      "Iteration: 635\n",
      "Training Loss: \n",
      "0.00076154243157\n",
      "\n",
      "Iteration: 636\n",
      "Training Loss: \n",
      "0.000896403674119\n",
      "\n",
      "Iteration: 637\n",
      "Training Loss: \n",
      "0.000637094520486\n",
      "\n",
      "Iteration: 638\n",
      "Training Loss: \n",
      "0.000295229165759\n",
      "\n",
      "Iteration: 639\n",
      "Training Loss: \n",
      "9.0895579969e-05\n",
      "\n",
      "Iteration: 640\n",
      "Training Loss: \n",
      "0.000113022261058\n",
      "\n",
      "Iteration: 641\n",
      "Training Loss: \n",
      "0.000329036666398\n",
      "\n",
      "Iteration: 642\n",
      "Training Loss: \n",
      "0.000221409866047\n",
      "\n",
      "Iteration: 643\n",
      "Training Loss: \n",
      "0.000112298253713\n",
      "\n",
      "Iteration: 644\n",
      "Training Loss: \n",
      "0.000405649580344\n",
      "\n",
      "Iteration: 645\n",
      "Training Loss: \n",
      "0.00130323942309\n",
      "\n",
      "Iteration: 646\n",
      "Training Loss: \n",
      "0.00208121910117\n",
      "\n",
      "Iteration: 647\n",
      "Training Loss: \n",
      "0.00228976120301\n",
      "\n",
      "Iteration: 648\n",
      "Training Loss: \n",
      "0.00283849634637\n",
      "\n",
      "Iteration: 649\n",
      "Training Loss: \n",
      "0.00239764272993\n",
      "\n",
      "Iteration: 650\n",
      "Training Loss: \n",
      "0.00173106455382\n",
      "\n",
      "Iteration: 651\n",
      "Training Loss: \n",
      "0.000865162996621\n",
      "\n",
      "Iteration: 652\n",
      "Training Loss: \n",
      "0.000254521302702\n",
      "\n",
      "Iteration: 653\n",
      "Training Loss: \n",
      "5.57644687191e-05\n",
      "\n",
      "Iteration: 654\n",
      "Training Loss: \n",
      "0.000291081592578\n",
      "\n",
      "Iteration: 655\n",
      "Training Loss: \n",
      "0.000759245191834\n",
      "\n",
      "Iteration: 656\n",
      "Training Loss: \n",
      "0.000514655394066\n",
      "\n",
      "Iteration: 657\n",
      "Training Loss: \n",
      "0.000134549888187\n",
      "\n",
      "Iteration: 658\n",
      "Training Loss: \n",
      "0.00010154749074\n",
      "\n",
      "Iteration: 659\n",
      "Training Loss: \n",
      "0.000293938287665\n",
      "\n",
      "Iteration: 660\n",
      "Training Loss: \n",
      "0.000588573913228\n",
      "\n",
      "Iteration: 661\n",
      "Training Loss: \n",
      "0.000848917283771\n",
      "\n",
      "Iteration: 662\n",
      "Training Loss: \n",
      "0.000969323276844\n",
      "\n",
      "Iteration: 663\n",
      "Training Loss: \n",
      "0.000846493773941\n",
      "\n",
      "Iteration: 664\n",
      "Training Loss: \n",
      "0.000456988237719\n",
      "\n",
      "Iteration: 665\n",
      "Training Loss: \n",
      "0.000232253263138\n",
      "\n",
      "Iteration: 666\n",
      "Training Loss: \n",
      "0.000179827596945\n",
      "\n",
      "Iteration: 667\n",
      "Training Loss: \n",
      "0.000229270082293\n",
      "\n",
      "Iteration: 668\n",
      "Training Loss: \n",
      "0.000290588209786\n",
      "\n",
      "Iteration: 669\n",
      "Training Loss: \n",
      "0.000228707322714\n",
      "\n",
      "Iteration: 670\n",
      "Training Loss: \n",
      "0.000404193914374\n",
      "\n",
      "Iteration: 671\n",
      "Training Loss: \n",
      "0.000972337395319\n",
      "\n",
      "Iteration: 672\n",
      "Training Loss: \n",
      "0.00137439841117\n",
      "\n",
      "Iteration: 673\n",
      "Training Loss: \n",
      "0.00143279743722\n",
      "\n",
      "Iteration: 674\n",
      "Training Loss: \n",
      "0.00126749153624\n",
      "\n",
      "Iteration: 675\n",
      "Training Loss: \n",
      "0.00144589991958\n",
      "\n",
      "Iteration: 676\n",
      "Training Loss: \n",
      "0.00126641438494\n",
      "\n",
      "Iteration: 677\n",
      "Training Loss: \n",
      "0.000766361722448\n",
      "\n",
      "Iteration: 678\n",
      "Training Loss: \n",
      "0.000697646275233\n",
      "\n",
      "Iteration: 679\n",
      "Training Loss: \n",
      "0.000584424507972\n",
      "\n",
      "Iteration: 680\n",
      "Training Loss: \n",
      "0.000464850392816\n",
      "\n",
      "Iteration: 681\n",
      "Training Loss: \n",
      "0.00030778026764\n",
      "\n",
      "Iteration: 682\n",
      "Training Loss: \n",
      "0.000245696238367\n",
      "\n",
      "Iteration: 683\n",
      "Training Loss: \n",
      "0.000336390701408\n",
      "\n",
      "Iteration: 684\n",
      "Training Loss: \n",
      "0.000349177842963\n",
      "\n",
      "Iteration: 685\n",
      "Training Loss: \n",
      "0.000431649600797\n",
      "\n",
      "Iteration: 686\n",
      "Training Loss: \n",
      "0.000500804556961\n",
      "\n",
      "Iteration: 687\n",
      "Training Loss: \n",
      "0.000650953430043\n",
      "\n",
      "Iteration: 688\n",
      "Training Loss: \n",
      "0.000327203562633\n",
      "\n",
      "Iteration: 689\n",
      "Training Loss: \n",
      "0.000223874205124\n",
      "\n",
      "Iteration: 690\n",
      "Training Loss: \n",
      "0.00027713662752\n",
      "\n",
      "Iteration: 691\n",
      "Training Loss: \n",
      "0.000346673308629\n",
      "\n",
      "Iteration: 692\n",
      "Training Loss: \n",
      "0.000352403036904\n",
      "\n",
      "Iteration: 693\n",
      "Training Loss: \n",
      "0.000328669282945\n",
      "\n",
      "Iteration: 694\n",
      "Training Loss: \n",
      "0.00045429128476\n",
      "\n",
      "Iteration: 695\n",
      "Training Loss: \n",
      "0.000713455835969\n",
      "\n",
      "Iteration: 696\n",
      "Training Loss: \n",
      "0.000877772494595\n",
      "\n",
      "Iteration: 697\n",
      "Training Loss: \n",
      "0.000857182393992\n",
      "\n",
      "Iteration: 698\n",
      "Training Loss: \n",
      "0.000758022260088\n",
      "\n",
      "Iteration: 699\n",
      "Training Loss: \n",
      "0.000735799724532\n",
      "\n",
      "Iteration: 700\n",
      "Training Loss: \n",
      "0.000648548042154\n",
      "\n",
      "Iteration: 701\n",
      "Training Loss: \n",
      "0.000481891825308\n",
      "\n",
      "Iteration: 702\n",
      "Training Loss: \n",
      "0.000351110683848\n",
      "\n",
      "Iteration: 703\n",
      "Training Loss: \n",
      "0.000375195669513\n",
      "\n",
      "Iteration: 704\n",
      "Training Loss: \n",
      "0.000363806385437\n",
      "\n",
      "Iteration: 705\n",
      "Training Loss: \n",
      "0.000247137758784\n",
      "\n",
      "Iteration: 706\n",
      "Training Loss: \n",
      "0.00020467400578\n",
      "\n",
      "Iteration: 707\n",
      "Training Loss: \n",
      "0.000448823154898\n",
      "\n",
      "Iteration: 708\n",
      "Training Loss: \n",
      "0.00082237729927\n",
      "\n",
      "Iteration: 709\n",
      "Training Loss: \n",
      "0.000979593220458\n",
      "\n",
      "Iteration: 710\n",
      "Training Loss: \n",
      "0.00148280571355\n",
      "\n",
      "Iteration: 711\n",
      "Training Loss: \n",
      "0.00167207588453\n",
      "\n",
      "Iteration: 712\n",
      "Training Loss: \n",
      "0.0013742450266\n",
      "\n",
      "Iteration: 713\n",
      "Training Loss: \n",
      "0.00105061555508\n",
      "\n",
      "Iteration: 714\n",
      "Training Loss: \n",
      "0.000541203247662\n",
      "\n",
      "Iteration: 715\n",
      "Training Loss: \n",
      "0.000282335502991\n",
      "\n",
      "Iteration: 716\n",
      "Training Loss: \n",
      "0.000115087953736\n",
      "\n",
      "Iteration: 717\n",
      "Training Loss: \n",
      "0.000100060580807\n",
      "\n",
      "Iteration: 718\n",
      "Training Loss: \n",
      "0.00017703256632\n",
      "\n",
      "Iteration: 719\n",
      "Training Loss: \n",
      "0.000414226198393\n",
      "\n",
      "Iteration: 720\n",
      "Training Loss: \n",
      "0.000251337654766\n",
      "\n",
      "Iteration: 721\n",
      "Training Loss: \n",
      "0.000128186022294\n",
      "\n",
      "Iteration: 722\n",
      "Training Loss: \n",
      "0.000113128132283\n",
      "\n",
      "Iteration: 723\n",
      "Training Loss: \n",
      "0.000150211478135\n",
      "\n",
      "Iteration: 724\n",
      "Training Loss: \n",
      "0.000166875501399\n",
      "\n",
      "Iteration: 725\n",
      "Training Loss: \n",
      "0.000161418408071\n",
      "\n",
      "Iteration: 726\n",
      "Training Loss: \n",
      "0.000175827863372\n",
      "\n",
      "Iteration: 727\n",
      "Training Loss: \n",
      "0.000219703180404\n",
      "\n",
      "Iteration: 728\n",
      "Training Loss: \n",
      "0.000177567872909\n",
      "\n",
      "Iteration: 729\n",
      "Training Loss: \n",
      "0.000159556580879\n",
      "\n",
      "Iteration: 730\n",
      "Training Loss: \n",
      "0.000251922030603\n",
      "\n",
      "Iteration: 731\n",
      "Training Loss: \n",
      "0.000951734274516\n",
      "\n",
      "Iteration: 732\n",
      "Training Loss: \n",
      "0.00173082916322\n",
      "\n",
      "Iteration: 733\n",
      "Training Loss: \n",
      "0.00197000039614\n",
      "\n",
      "Iteration: 734\n",
      "Training Loss: \n",
      "0.00159577184967\n",
      "\n",
      "Iteration: 735\n",
      "Training Loss: \n",
      "0.00088490586828\n",
      "\n",
      "Iteration: 736\n",
      "Training Loss: \n",
      "0.000468264718604\n",
      "\n",
      "Iteration: 737\n",
      "Training Loss: \n",
      "0.000193424116594\n",
      "\n",
      "Iteration: 738\n",
      "Training Loss: \n",
      "0.000225958477715\n",
      "\n",
      "Iteration: 739\n",
      "Training Loss: \n",
      "0.000267396243726\n",
      "\n",
      "Iteration: 740\n",
      "Training Loss: \n",
      "0.000144283340348\n",
      "\n",
      "Iteration: 741\n",
      "Training Loss: \n",
      "7.13984696955e-05\n",
      "\n",
      "Iteration: 742\n",
      "Training Loss: \n",
      "0.000175523602699\n",
      "\n",
      "Iteration: 743\n",
      "Training Loss: \n",
      "0.000503502615853\n",
      "\n",
      "Iteration: 744\n",
      "Training Loss: \n",
      "0.000811390861129\n",
      "\n",
      "Iteration: 745\n",
      "Training Loss: \n",
      "0.000697144232598\n",
      "\n",
      "Iteration: 746\n",
      "Training Loss: \n",
      "0.000305270285574\n",
      "\n",
      "Iteration: 747\n",
      "Training Loss: \n",
      "9.09397483723e-05\n",
      "\n",
      "Iteration: 748\n",
      "Training Loss: \n",
      "0.000109964540032\n",
      "\n",
      "Iteration: 749\n",
      "Training Loss: \n",
      "0.00032519019517\n",
      "\n",
      "Iteration: 750\n",
      "Training Loss: \n",
      "0.000204335189412\n",
      "\n",
      "Iteration: 751\n",
      "Training Loss: \n",
      "0.000113428740681\n",
      "\n",
      "Iteration: 752\n",
      "Training Loss: \n",
      "0.000117326774868\n",
      "\n",
      "Iteration: 753\n",
      "Training Loss: \n",
      "0.000216310016576\n",
      "\n",
      "Iteration: 754\n",
      "Training Loss: \n",
      "0.000301113219264\n",
      "\n",
      "Iteration: 755\n",
      "Training Loss: \n",
      "0.000323516876287\n",
      "\n",
      "Iteration: 756\n",
      "Training Loss: \n",
      "0.000295575275297\n",
      "\n",
      "Iteration: 757\n",
      "Training Loss: \n",
      "0.00051254864137\n",
      "\n",
      "Iteration: 758\n",
      "Training Loss: \n",
      "0.000605426791514\n",
      "\n",
      "Iteration: 759\n",
      "Training Loss: \n",
      "0.000524066629484\n",
      "\n",
      "Iteration: 760\n",
      "Training Loss: \n",
      "0.000715268680167\n",
      "\n",
      "Iteration: 761\n",
      "Training Loss: \n",
      "0.000681797465884\n",
      "\n",
      "Iteration: 762\n",
      "Training Loss: \n",
      "0.000495059476307\n",
      "\n",
      "Iteration: 763\n",
      "Training Loss: \n",
      "0.000343591081915\n",
      "\n",
      "Iteration: 764\n",
      "Training Loss: \n",
      "0.000417140564602\n",
      "\n",
      "Iteration: 765\n",
      "Training Loss: \n",
      "0.00053984530312\n",
      "\n",
      "Iteration: 766\n",
      "Training Loss: \n",
      "0.000593142645118\n",
      "\n",
      "Iteration: 767\n",
      "Training Loss: \n",
      "0.000570017331365\n",
      "\n",
      "Iteration: 768\n",
      "Training Loss: \n",
      "0.000525398233434\n",
      "\n",
      "Iteration: 769\n",
      "Training Loss: \n",
      "0.000530351037212\n",
      "\n",
      "Iteration: 770\n",
      "Training Loss: \n",
      "0.000610910815772\n",
      "\n",
      "Iteration: 771\n",
      "Training Loss: \n",
      "0.000747150062668\n",
      "\n",
      "Iteration: 772\n",
      "Training Loss: \n",
      "0.000992211072137\n",
      "\n",
      "Iteration: 773\n",
      "Training Loss: \n",
      "0.0011448279688\n",
      "\n",
      "Iteration: 774\n",
      "Training Loss: \n",
      "0.00123103009321\n",
      "\n",
      "Iteration: 775\n",
      "Training Loss: \n",
      "0.00135773675993\n",
      "\n",
      "Iteration: 776\n",
      "Training Loss: \n",
      "0.00121073902509\n",
      "\n",
      "Iteration: 777\n",
      "Training Loss: \n",
      "0.000622893602175\n",
      "\n",
      "Iteration: 778\n",
      "Training Loss: \n",
      "0.000705720627998\n",
      "\n",
      "Iteration: 779\n",
      "Training Loss: \n",
      "0.00149760620756\n",
      "\n",
      "Iteration: 780\n",
      "Training Loss: \n",
      "0.00210108104093\n",
      "\n",
      "Iteration: 781\n",
      "Training Loss: \n",
      "0.00281921085869\n",
      "\n",
      "Iteration: 782\n",
      "Training Loss: \n",
      "0.00284979953966\n",
      "\n",
      "Iteration: 783\n",
      "Training Loss: \n",
      "0.00236635703275\n",
      "\n",
      "Iteration: 784\n",
      "Training Loss: \n",
      "0.00145437689111\n",
      "\n",
      "Iteration: 785\n",
      "Training Loss: \n",
      "0.000935713608956\n",
      "\n",
      "Iteration: 786\n",
      "Training Loss: \n",
      "0.000481793488242\n",
      "\n",
      "Iteration: 787\n",
      "Training Loss: \n",
      "0.000266355851079\n",
      "\n",
      "Iteration: 788\n",
      "Training Loss: \n",
      "0.000333593205301\n",
      "\n",
      "Iteration: 789\n",
      "Training Loss: \n",
      "0.000555912297967\n",
      "\n",
      "Iteration: 790\n",
      "Training Loss: \n",
      "0.000436727504512\n",
      "\n",
      "Iteration: 791\n",
      "Training Loss: \n",
      "0.000248324316782\n",
      "\n",
      "Iteration: 792\n",
      "Training Loss: \n",
      "0.000330041470798\n",
      "\n",
      "Iteration: 793\n",
      "Training Loss: \n",
      "0.000517378737426\n",
      "\n",
      "Iteration: 794\n",
      "Training Loss: \n",
      "0.000603915827794\n",
      "\n",
      "Iteration: 795\n",
      "Training Loss: \n",
      "0.000912169252421\n",
      "\n",
      "Iteration: 796\n",
      "Training Loss: \n",
      "0.00102955642196\n",
      "\n",
      "Iteration: 797\n",
      "Training Loss: \n",
      "0.00131132931135\n",
      "\n",
      "Iteration: 798\n",
      "Training Loss: \n",
      "0.00126503023282\n",
      "\n",
      "Iteration: 799\n",
      "Training Loss: \n",
      "0.00086954290232\n",
      "\n",
      "Iteration: 800\n",
      "Training Loss: \n",
      "0.000560154739283\n",
      "\n",
      "Iteration: 801\n",
      "Training Loss: \n",
      "0.000501945817222\n",
      "\n",
      "Iteration: 802\n",
      "Training Loss: \n",
      "0.000399330076495\n",
      "\n",
      "Iteration: 803\n",
      "Training Loss: \n",
      "0.000268314121429\n",
      "\n",
      "Iteration: 804\n",
      "Training Loss: \n",
      "0.000201819554948\n",
      "\n",
      "Iteration: 805\n",
      "Training Loss: \n",
      "0.000249179477661\n",
      "\n",
      "Iteration: 806\n",
      "Training Loss: \n",
      "0.000236772739713\n",
      "\n",
      "Iteration: 807\n",
      "Training Loss: \n",
      "0.000211560432736\n",
      "\n",
      "Iteration: 808\n",
      "Training Loss: \n",
      "0.000204547120541\n",
      "\n",
      "Iteration: 809\n",
      "Training Loss: \n",
      "0.000215851831848\n",
      "\n",
      "Iteration: 810\n",
      "Training Loss: \n",
      "0.000126918882668\n",
      "\n",
      "Iteration: 811\n",
      "Training Loss: \n",
      "0.000278647614715\n",
      "\n",
      "Iteration: 812\n",
      "Training Loss: \n",
      "0.000625559951019\n",
      "\n",
      "Iteration: 813\n",
      "Training Loss: \n",
      "0.000848143299092\n",
      "\n",
      "Iteration: 814\n",
      "Training Loss: \n",
      "0.000805592788835\n",
      "\n",
      "Iteration: 815\n",
      "Training Loss: \n",
      "0.000938709915133\n",
      "\n",
      "Iteration: 816\n",
      "Training Loss: \n",
      "0.000787589246544\n",
      "\n",
      "Iteration: 817\n",
      "Training Loss: \n",
      "0.000465526088852\n",
      "\n",
      "Iteration: 818\n",
      "Training Loss: \n",
      "0.0001858280809\n",
      "\n",
      "Iteration: 819\n",
      "Training Loss: \n",
      "7.00932959961e-05\n",
      "\n",
      "Iteration: 820\n",
      "Training Loss: \n",
      "0.000187758908305\n",
      "\n",
      "Iteration: 821\n",
      "Training Loss: \n",
      "0.000501615051686\n",
      "\n",
      "Iteration: 822\n",
      "Training Loss: \n",
      "0.000789406559691\n",
      "\n",
      "Iteration: 823\n",
      "Training Loss: \n",
      "0.00101540371221\n",
      "\n",
      "Iteration: 824\n",
      "Training Loss: \n",
      "0.00111988497702\n",
      "\n",
      "Iteration: 825\n",
      "Training Loss: \n",
      "0.00117446752453\n",
      "\n",
      "Iteration: 826\n",
      "Training Loss: \n",
      "0.000943092566485\n",
      "\n",
      "Iteration: 827\n",
      "Training Loss: \n",
      "0.000739808412322\n",
      "\n",
      "Iteration: 828\n",
      "Training Loss: \n",
      "0.000444813683993\n",
      "\n",
      "Iteration: 829\n",
      "Training Loss: \n",
      "5.70067947356e-05\n",
      "\n",
      "Iteration: 830\n",
      "Training Loss: \n",
      "0.000371114050803\n",
      "\n",
      "Iteration: 831\n",
      "Training Loss: \n",
      "0.00157569862988\n",
      "\n",
      "Iteration: 832\n",
      "Training Loss: \n",
      "0.0027599016572\n",
      "\n",
      "Iteration: 833\n",
      "Training Loss: \n",
      "0.00288965473234\n",
      "\n",
      "Iteration: 834\n",
      "Training Loss: \n",
      "0.00224968081946\n",
      "\n",
      "Iteration: 835\n",
      "Training Loss: \n",
      "0.00122877689141\n",
      "\n",
      "Iteration: 836\n",
      "Training Loss: \n",
      "0.000363382930826\n",
      "\n",
      "Iteration: 837\n",
      "Training Loss: \n",
      "4.60767762967e-05\n",
      "\n",
      "Iteration: 838\n",
      "Training Loss: \n",
      "0.000490623137477\n",
      "\n",
      "Iteration: 839\n",
      "Training Loss: \n",
      "0.000741017282637\n",
      "\n",
      "Iteration: 840\n",
      "Training Loss: \n",
      "0.000511801824526\n",
      "\n",
      "Iteration: 841\n",
      "Training Loss: \n",
      "0.000170134274737\n",
      "\n",
      "Iteration: 842\n",
      "Training Loss: \n",
      "0.000199620172423\n",
      "\n",
      "Iteration: 843\n",
      "Training Loss: \n",
      "0.000634799094459\n",
      "\n",
      "Iteration: 844\n",
      "Training Loss: \n",
      "0.00167574195891\n",
      "\n",
      "Iteration: 845\n",
      "Training Loss: \n",
      "0.00242166625459\n",
      "\n",
      "Iteration: 846\n",
      "Training Loss: \n",
      "0.00296562706397\n",
      "\n",
      "Iteration: 847\n",
      "Training Loss: \n",
      "0.00259924556442\n",
      "\n",
      "Iteration: 848\n",
      "Training Loss: \n",
      "0.00171819812753\n",
      "\n",
      "Iteration: 849\n",
      "Training Loss: \n",
      "0.00105734230503\n",
      "\n",
      "Iteration: 850\n",
      "Training Loss: \n",
      "0.000684128835035\n",
      "\n",
      "Iteration: 851\n",
      "Training Loss: \n",
      "0.000548087772661\n",
      "\n",
      "Iteration: 852\n",
      "Training Loss: \n",
      "0.000424975718917\n",
      "\n",
      "Iteration: 853\n",
      "Training Loss: \n",
      "0.000243093661848\n",
      "\n",
      "Iteration: 854\n",
      "Training Loss: \n",
      "0.000167396943907\n",
      "\n",
      "Iteration: 855\n",
      "Training Loss: \n",
      "0.000106333986554\n",
      "\n",
      "Iteration: 856\n",
      "Training Loss: \n",
      "0.000102501036407\n",
      "\n",
      "Iteration: 857\n",
      "Training Loss: \n",
      "8.174331822e-05\n",
      "\n",
      "Iteration: 858\n",
      "Training Loss: \n",
      "7.69666496473e-05\n",
      "\n",
      "Iteration: 859\n",
      "Training Loss: \n",
      "6.6605774923e-05\n",
      "\n",
      "Iteration: 860\n",
      "Training Loss: \n",
      "6.03471380391e-05\n",
      "\n",
      "Iteration: 861\n",
      "Training Loss: \n",
      "6.00051710209e-05\n",
      "\n",
      "Iteration: 862\n",
      "Training Loss: \n",
      "7.26976724035e-05\n",
      "\n",
      "Iteration: 863\n",
      "Training Loss: \n",
      "9.98551801801e-05\n",
      "\n",
      "Iteration: 864\n",
      "Training Loss: \n",
      "0.000136274018236\n",
      "\n",
      "Iteration: 865\n",
      "Training Loss: \n",
      "0.000171195364216\n",
      "\n",
      "Iteration: 866\n",
      "Training Loss: \n",
      "3.55054083999e-05\n",
      "\n",
      "Iteration: 867\n",
      "Training Loss: \n",
      "0.000147552220588\n",
      "\n",
      "Iteration: 868\n",
      "Training Loss: \n",
      "0.000900943949631\n",
      "\n",
      "Iteration: 869\n",
      "Training Loss: \n",
      "0.00167499613086\n",
      "\n",
      "Iteration: 870\n",
      "Training Loss: \n",
      "0.00210858185937\n",
      "\n",
      "Iteration: 871\n",
      "Training Loss: \n",
      "0.00191715359138\n",
      "\n",
      "Iteration: 872\n",
      "Training Loss: \n",
      "0.0012465803588\n",
      "\n",
      "Iteration: 873\n",
      "Training Loss: \n",
      "0.000447680480398\n",
      "\n",
      "Iteration: 874\n",
      "Training Loss: \n",
      "6.38416973574e-05\n",
      "\n",
      "Iteration: 875\n",
      "Training Loss: \n",
      "0.000301912303324\n",
      "\n",
      "Iteration: 876\n",
      "Training Loss: \n",
      "0.00106874646114\n",
      "\n",
      "Iteration: 877\n",
      "Training Loss: \n",
      "0.00204473395105\n",
      "\n",
      "Iteration: 878\n",
      "Training Loss: \n",
      "0.00168955792106\n",
      "\n",
      "Iteration: 879\n",
      "Training Loss: \n",
      "0.00104457211844\n",
      "\n",
      "Iteration: 880\n",
      "Training Loss: \n",
      "0.000464629066315\n",
      "\n",
      "Iteration: 881\n",
      "Training Loss: \n",
      "0.000207128132477\n",
      "\n",
      "Iteration: 882\n",
      "Training Loss: \n",
      "0.000269861566768\n",
      "\n",
      "Iteration: 883\n",
      "Training Loss: \n",
      "0.000495934009125\n",
      "\n",
      "Iteration: 884\n",
      "Training Loss: \n",
      "0.000670166092999\n",
      "\n",
      "Iteration: 885\n",
      "Training Loss: \n",
      "0.000703167647646\n",
      "\n",
      "Iteration: 886\n",
      "Training Loss: \n",
      "0.000514974489602\n",
      "\n",
      "Iteration: 887\n",
      "Training Loss: \n",
      "0.000296434636168\n",
      "\n",
      "Iteration: 888\n",
      "Training Loss: \n",
      "0.00025181300349\n",
      "\n",
      "Iteration: 889\n",
      "Training Loss: \n",
      "0.000254284044978\n",
      "\n",
      "Iteration: 890\n",
      "Training Loss: \n",
      "0.000193672417015\n",
      "\n",
      "Iteration: 891\n",
      "Training Loss: \n",
      "0.00010810032115\n",
      "\n",
      "Iteration: 892\n",
      "Training Loss: \n",
      "6.90387442523e-05\n",
      "\n",
      "Iteration: 893\n",
      "Training Loss: \n",
      "8.67078802077e-05\n",
      "\n",
      "Iteration: 894\n",
      "Training Loss: \n",
      "0.000117908739116\n",
      "\n",
      "Iteration: 895\n",
      "Training Loss: \n",
      "0.000215601420108\n",
      "\n",
      "Iteration: 896\n",
      "Training Loss: \n",
      "0.000529735925738\n",
      "\n",
      "Iteration: 897\n",
      "Training Loss: \n",
      "0.000727485394465\n",
      "\n",
      "Iteration: 898\n",
      "Training Loss: \n",
      "0.000737749795819\n",
      "\n",
      "Iteration: 899\n",
      "Training Loss: \n",
      "0.000676749180248\n",
      "\n",
      "Iteration: 900\n",
      "Training Loss: \n",
      "0.000394919359115\n",
      "\n",
      "Iteration: 901\n",
      "Training Loss: \n",
      "0.000123718920975\n",
      "\n",
      "Iteration: 902\n",
      "Training Loss: \n",
      "2.54270231728e-05\n",
      "\n",
      "Iteration: 903\n",
      "Training Loss: \n",
      "0.000157357403979\n",
      "\n",
      "Iteration: 904\n",
      "Training Loss: \n",
      "0.000439689288171\n",
      "\n",
      "Iteration: 905\n",
      "Training Loss: \n",
      "0.000713109181025\n",
      "\n",
      "Iteration: 906\n",
      "Training Loss: \n",
      "0.000720753879076\n",
      "\n",
      "Iteration: 907\n",
      "Training Loss: \n",
      "0.000717649543264\n",
      "\n",
      "Iteration: 908\n",
      "Training Loss: \n",
      "0.00047725114823\n",
      "\n",
      "Iteration: 909\n",
      "Training Loss: \n",
      "6.05337182161e-05\n",
      "\n",
      "Iteration: 910\n",
      "Training Loss: \n",
      "0.000117028148235\n",
      "\n",
      "Iteration: 911\n",
      "Training Loss: \n",
      "0.000548179476341\n",
      "\n",
      "Iteration: 912\n",
      "Training Loss: \n",
      "0.00143962382137\n",
      "\n",
      "Iteration: 913\n",
      "Training Loss: \n",
      "0.00251730580662\n",
      "\n",
      "Iteration: 914\n",
      "Training Loss: \n",
      "0.00268131346487\n",
      "\n",
      "Iteration: 915\n",
      "Training Loss: \n",
      "0.00236273561905\n",
      "\n",
      "Iteration: 916\n",
      "Training Loss: \n",
      "0.00149329415214\n",
      "\n",
      "Iteration: 917\n",
      "Training Loss: \n",
      "0.000592127630066\n",
      "\n",
      "Iteration: 918\n",
      "Training Loss: \n",
      "0.00026871073166\n",
      "\n",
      "Iteration: 919\n",
      "Training Loss: \n",
      "0.000263929405677\n",
      "\n",
      "Iteration: 920\n",
      "Training Loss: \n",
      "0.000385022098651\n",
      "\n",
      "Iteration: 921\n",
      "Training Loss: \n",
      "0.00052129277865\n",
      "\n",
      "Iteration: 922\n",
      "Training Loss: \n",
      "0.000699540430957\n",
      "\n",
      "Iteration: 923\n",
      "Training Loss: \n",
      "0.000546677071294\n",
      "\n",
      "Iteration: 924\n",
      "Training Loss: \n",
      "0.00042509216253\n",
      "\n",
      "Iteration: 925\n",
      "Training Loss: \n",
      "0.000462341123041\n",
      "\n",
      "Iteration: 926\n",
      "Training Loss: \n",
      "0.000994171744999\n",
      "\n",
      "Iteration: 927\n",
      "Training Loss: \n",
      "0.00170837824558\n",
      "\n",
      "Iteration: 928\n",
      "Training Loss: \n",
      "0.00220865931653\n",
      "\n",
      "Iteration: 929\n",
      "Training Loss: \n",
      "0.00209692069481\n",
      "\n",
      "Iteration: 930\n",
      "Training Loss: \n",
      "0.0014200406214\n",
      "\n",
      "Iteration: 931\n",
      "Training Loss: \n",
      "0.000748311525493\n",
      "\n",
      "Iteration: 932\n",
      "Training Loss: \n",
      "0.000475071797282\n",
      "\n",
      "Iteration: 933\n",
      "Training Loss: \n",
      "0.000390311768131\n",
      "\n",
      "Iteration: 934\n",
      "Training Loss: \n",
      "0.000473845963851\n",
      "\n",
      "Iteration: 935\n",
      "Training Loss: \n",
      "0.000419937420579\n",
      "\n",
      "Iteration: 936\n",
      "Training Loss: \n",
      "0.000363534432409\n",
      "\n",
      "Iteration: 937\n",
      "Training Loss: \n",
      "0.000199204203594\n",
      "\n",
      "Iteration: 938\n",
      "Training Loss: \n",
      "0.000221687928836\n",
      "\n",
      "Iteration: 939\n",
      "Training Loss: \n",
      "0.000495614645658\n",
      "\n",
      "Iteration: 940\n",
      "Training Loss: \n",
      "0.000708371331389\n",
      "\n",
      "Iteration: 941\n",
      "Training Loss: \n",
      "0.000830087659655\n",
      "\n",
      "Iteration: 942\n",
      "Training Loss: \n",
      "0.000719200489275\n",
      "\n",
      "Iteration: 943\n",
      "Training Loss: \n",
      "0.000544569080232\n",
      "\n",
      "Iteration: 944\n",
      "Training Loss: \n",
      "0.000333284597731\n",
      "\n",
      "Iteration: 945\n",
      "Training Loss: \n",
      "0.000230798593033\n",
      "\n",
      "Iteration: 946\n",
      "Training Loss: \n",
      "0.000228245036813\n",
      "\n",
      "Iteration: 947\n",
      "Training Loss: \n",
      "0.000256095732448\n",
      "\n",
      "Iteration: 948\n",
      "Training Loss: \n",
      "0.000278716319947\n",
      "\n",
      "Iteration: 949\n",
      "Training Loss: \n",
      "0.000349231672967\n",
      "\n",
      "Iteration: 950\n",
      "Training Loss: \n",
      "0.000203509344355\n",
      "\n",
      "Iteration: 951\n",
      "Training Loss: \n",
      "0.00014608064106\n",
      "\n",
      "Iteration: 952\n",
      "Training Loss: \n",
      "0.000325291552769\n",
      "\n",
      "Iteration: 953\n",
      "Training Loss: \n",
      "0.000824563601028\n",
      "\n",
      "Iteration: 954\n",
      "Training Loss: \n",
      "0.00162817082193\n",
      "\n",
      "Iteration: 955\n",
      "Training Loss: \n",
      "0.00203104590256\n",
      "\n",
      "Iteration: 956\n",
      "Training Loss: \n",
      "0.00186265255766\n",
      "\n",
      "Iteration: 957\n",
      "Training Loss: \n",
      "0.00120022145494\n",
      "\n",
      "Iteration: 958\n",
      "Training Loss: \n",
      "0.000669957687949\n",
      "\n",
      "Iteration: 959\n",
      "Training Loss: \n",
      "0.000459303449371\n",
      "\n",
      "Iteration: 960\n",
      "Training Loss: \n",
      "0.000412496710751\n",
      "\n",
      "Iteration: 961\n",
      "Training Loss: \n",
      "0.000377110263789\n",
      "\n",
      "Iteration: 962\n",
      "Training Loss: \n",
      "0.000444036453775\n",
      "\n",
      "Iteration: 963\n",
      "Training Loss: \n",
      "0.000687690451372\n",
      "\n",
      "Iteration: 964\n",
      "Training Loss: \n",
      "0.000627053699108\n",
      "\n",
      "Iteration: 965\n",
      "Training Loss: \n",
      "0.000308086051891\n",
      "\n",
      "Iteration: 966\n",
      "Training Loss: \n",
      "0.000319328590319\n",
      "\n",
      "Iteration: 967\n",
      "Training Loss: \n",
      "0.00065256499894\n",
      "\n",
      "Iteration: 968\n",
      "Training Loss: \n",
      "0.001021958797\n",
      "\n",
      "Iteration: 969\n",
      "Training Loss: \n",
      "0.00114289998359\n",
      "\n",
      "Iteration: 970\n",
      "Training Loss: \n",
      "0.00106239051464\n",
      "\n",
      "Iteration: 971\n",
      "Training Loss: \n",
      "0.000731151579246\n",
      "\n",
      "Iteration: 972\n",
      "Training Loss: \n",
      "0.000304567029842\n",
      "\n",
      "Iteration: 973\n",
      "Training Loss: \n",
      "3.54935116938e-05\n",
      "\n",
      "Iteration: 974\n",
      "Training Loss: \n",
      "6.22990469956e-05\n",
      "\n",
      "Iteration: 975\n",
      "Training Loss: \n",
      "0.000367856541874\n",
      "\n",
      "Iteration: 976\n",
      "Training Loss: \n",
      "0.000626650500059\n",
      "\n",
      "Iteration: 977\n",
      "Training Loss: \n",
      "0.000470594298905\n",
      "\n",
      "Iteration: 978\n",
      "Training Loss: \n",
      "0.000290308212694\n",
      "\n",
      "Iteration: 979\n",
      "Training Loss: \n",
      "0.00015887958223\n",
      "\n",
      "Iteration: 980\n",
      "Training Loss: \n",
      "0.000116282644625\n",
      "\n",
      "Iteration: 981\n",
      "Training Loss: \n",
      "0.000348842641412\n",
      "\n",
      "Iteration: 982\n",
      "Training Loss: \n",
      "0.000508322963302\n",
      "\n",
      "Iteration: 983\n",
      "Training Loss: \n",
      "0.000437963948041\n",
      "\n",
      "Iteration: 984\n",
      "Training Loss: \n",
      "0.000320748960114\n",
      "\n",
      "Iteration: 985\n",
      "Training Loss: \n",
      "0.000204450040878\n",
      "\n",
      "Iteration: 986\n",
      "Training Loss: \n",
      "0.000129423926566\n",
      "\n",
      "Iteration: 987\n",
      "Training Loss: \n",
      "7.93785405712e-05\n",
      "\n",
      "Iteration: 988\n",
      "Training Loss: \n",
      "5.83113476423e-05\n",
      "\n",
      "Iteration: 989\n",
      "Training Loss: \n",
      "5.69681684601e-05\n",
      "\n",
      "Iteration: 990\n",
      "Training Loss: \n",
      "0.000115260957557\n",
      "\n",
      "Iteration: 991\n",
      "Training Loss: \n",
      "0.000243061448623\n",
      "\n",
      "Iteration: 992\n",
      "Training Loss: \n",
      "0.000605042463426\n",
      "\n",
      "Iteration: 993\n",
      "Training Loss: \n",
      "0.00077997101784\n",
      "\n",
      "Iteration: 994\n",
      "Training Loss: \n",
      "0.000734477056309\n",
      "\n",
      "Iteration: 995\n",
      "Training Loss: \n",
      "0.000749621116123\n",
      "\n",
      "Iteration: 996\n",
      "Training Loss: \n",
      "0.000573997758414\n",
      "\n",
      "Iteration: 997\n",
      "Training Loss: \n",
      "0.000316116921867\n",
      "\n",
      "Iteration: 998\n",
      "Training Loss: \n",
      "0.000237726465208\n",
      "\n",
      "Iteration: 999\n",
      "Training Loss: \n",
      "0.000167354587972\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wb_vect, wb_unflattener = adam(grad_tl, wb_all, callback=clbk, num_iters=1000)\n",
    "wb_all = wb_unflattener(wb_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAECCAYAAAAciLtvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKAqA/w/LR3m/7GBXJAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1176250f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(training_losses)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['66668-0', '112387-1']\n",
      "Predictions:\n",
      "[[ 0.26481597]\n",
      " [ 0.94829197]]\n",
      "Mean: 0.6065539727816811\n",
      "\n",
      "Actual\n",
      "[[ 0.25527251]\n",
      " [ 0.93951925]]\n",
      "Mean: 0.5973958788609622\n",
      "\n",
      "Difference\n",
      "[[ 0.00954347]\n",
      " [ 0.00877272]]\n",
      "Mean Squared Error: 8.40191984243525e-05\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8.4019198424352507e-05"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from graphfp.flatten import flatten\n",
    "wb_vect, wb_unflattener = flatten(wb_all)\n",
    "train_loss(wb_vect, wb_unflattener, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18730440864022327"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wb_all['layer0_GraphConvLayer']['weights'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  },
  "widgets": {
   "state": {},
   "version": "1.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
